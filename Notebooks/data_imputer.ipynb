{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed42230",
   "metadata": {},
   "source": [
    "$$\\large \\text{Packages & Specs} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7499c534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 18:26:09.142008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 18:26:37.723587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "# Kalman Smoothing using R objects\n",
    "import rpy2.robjects as robjects\n",
    "# import R packages\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Impute TS\n",
    "imputeTS = importr('imputeTS')\n",
    "kalman_StructTs = robjects.r['na_kalman']\n",
    "\n",
    "module_path = re.sub(r'Notebooks|Python Scripts','Python Scripts',os.getcwd())\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from pv_modules import *\n",
    "from tensorflow import keras\n",
    "\n",
    "meteo_datapath = re.sub(r'Notebooks|Python Scripts','Support Files/',os.getcwd())\n",
    "# == Load Hourly Meteo Data == #\n",
    "meteo_file = 'hour_all_meteo_data_clean'\n",
    "meteo_df = pd.read_csv(meteo_datapath + f'{meteo_file}.csv',index_col=0)\n",
    "meteo_df.index = pd.to_datetime(meteo_df.index)\n",
    "# == Load Feature Data == #\n",
    "feature_file = 'all_meteo_data_clean_4D_4Y'\n",
    "features_df = pd.read_csv(meteo_datapath + f'{feature_file}.csv',index_col=0)\n",
    "features_df.index = pd.to_datetime(features_df.index)\n",
    "\n",
    "model_datapath = re.sub(r'Notebooks|Python Scripts','Python Scripts/',os.getcwd())\n",
    "nn_model = keras.models.load_model(model_datapath + f\"multivariate_mlp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4cb3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def interpolation_method(df, nan_gaps):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs interpolation on a DataFrame to fill missing values using the 'time' method.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        nan_gaps (dict): Dictionary containing column names as keys and lists of NaN gap indices as values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with filled values using time based linear interpolation.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    output_df = df.copy()\n",
    "        \n",
    "    df = df.interpolate(method='time', limit_direction='both')\n",
    "            \n",
    "    for col in nan_gaps.keys():\n",
    "        output_df[col].iloc[nan_gaps[col]] = df[col].iloc[nan_gaps[col]]\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d87ba9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ARIMA(df, nan_gaps):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies Kalman filtering to a DataFrame to fill missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        nan_gaps (dict): Dictionary containing column names as keys and lists of NaN gap indices as values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with filled values using Kalman filtering.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    output_df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = kalman_StructTs(arr, model = \"auto.arima\")\n",
    "        \n",
    "    for col in nan_gaps.keys():\n",
    "        output_df[col].iloc[nan_gaps[col]] = df[col].iloc[nan_gaps[col]]\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63b901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Regression(df):\n",
    "\n",
    "    nn_features = ml_df.loc[df.index[0]:df.index[-1],:]\n",
    "    nn_features = nn_features[nn_features.index.isin(df.index)]\n",
    "\n",
    "    X = nn_features.iloc[test_gaps[df.columns[0]]].to_numpy()\n",
    "        \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    imputation_data = model.predict(X_scaled)\n",
    "\n",
    "    df.iloc[test_gaps[df.columns[0]]] = imputation_data\n",
    "\n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee24105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fill_hour(df):\n",
    "    \"\"\"\n",
    "    Fill missing values in the df DataFrame using hourly data from the Grigy Site.\n",
    "    \n",
    "    Args:\n",
    "        test_df (pandas.DataFrame): The DataFrame containing the test data.\n",
    "        file (str): The type of file for which missing values need to be filled. \n",
    "                    Valid options are \"Irradiance\" or any other file type.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A modified df DataFrame with the missing hour values.\n",
    "    \"\"\"\n",
    "\n",
    "    fill_df = meteo_df.loc[df.index[0]:df.index[-1],:]\n",
    "    fill_df = fill_df[fill_df.index.isin(df.index)]\n",
    "\n",
    "    if file == \"Irradiance\":\n",
    "        df['DirectIR'] = df['DirectIR'].fillna(fill_df['Direct Shortwave Radiation (W/m²) (sfc)'])\n",
    "        df['DiffuseIR'] = df['DiffuseIR'].fillna(fill_df['Diffuse Shortwave Radiation (W/m²) (sfc)'])\n",
    "        df['Temperature'] = df['Temperature'].fillna(fill_df['Temperature (°C) (2 m elevation corrected)'])\n",
    "        df['WindSpeed'] = df['WindSpeed'].fillna(fill_df['Wind Speed (m/s) (10 m)'])\n",
    "    else:\n",
    "        print(f\"No available data to fill hourly gaps for file type: {file}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605d8363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_imputer(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds the index positions of gaps in a DataFrame based on their size and applies appropriate imputation method.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with filled missing values using interpolation and Kalman filtering.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    interpolation = {}\n",
    "    arima = {}\n",
    "    \n",
    "    for col in range(len(df.columns)):\n",
    "        index_list = []\n",
    "        interpolation[df.columns[col]] = []\n",
    "        arima[df.columns[col]] = []\n",
    "        for index in range(len(df.index)):\n",
    "            if index in index_list: continue\n",
    "            day = df.index[index].day\n",
    "            c = 0\n",
    "            while np.isnan(df.iloc[index+c,col]) and df.index[index+c].day == day:\n",
    "                if df.index[index+c] == df.index[-1]: break\n",
    "                index_list += [index+c]           \n",
    "                c += 1 \n",
    "            dt = (df.index[index+c] - df.index[index]).total_seconds()\n",
    "            if not c and not np.isnan(df.iloc[index+c,col]): continue\n",
    "                \n",
    "            # == uncomment block if removing night time values prior to imputing missing values == #\n",
    "#             elif df.index[index+c] == df.index[-1] and np.isnan(df.iloc[index+c,col]):\n",
    "#                 if dt <= 200:\n",
    "#                     interpolation[df.columns[col]] += list(range(index,index+c+1))\n",
    "#                 else:\n",
    "#                     arima[df.columns[col]] += list(range(index,index+c+1))\n",
    "#             elif df.index[index+c+1].day != df.index[index+c].day and np.isnan(df.iloc[index+c+1,col]):\n",
    "#                 arima[df.columns[col]] += list(range(index,index+c+1))\n",
    "#             elif df.index[index-1].day != df.index[index].day and np.isnan(df.iloc[index,col]):\n",
    "#                 arima[df.columns[col]] += list(range(index,index+c+1))\n",
    "            # == uncomment block if removing night time values prior to imputing missing values == #\n",
    "    \n",
    "            elif dt <= 200:\n",
    "                interpolation[df.columns[col]] += list(range(index,index+c+1))\n",
    "            else:\n",
    "                arima[df.columns[col]] += list(range(index,index+c+1))\n",
    "\n",
    "        if not interpolation[df.columns[col]]:\n",
    "            del interpolation[df.columns[col]]\n",
    "        if not arima[df.columns[col]]:\n",
    "            del arima[df.columns[col]]\n",
    "            \n",
    "    if interpolation:\n",
    "        df = interpolation_method(df,interpolation)\n",
    "    if arima:\n",
    "        df = ARIMA(df,arima)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7226f6",
   "metadata": {},
   "source": [
    "$$\\large \\text{Imputer; instance of df for cleaning and preprocessing} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e992a8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Imputer():\n",
    "    \n",
    "    \"\"\"\n",
    "    Class for data imputation and cleaning.\n",
    "    \n",
    "    Attributes:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        month (str): Month.\n",
    "        year (str): Year.\n",
    "        file (str): File type.\n",
    "\n",
    "    Methods:\n",
    "        run(): Runs the data imputation and cleaning process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,df,month,year,file):\n",
    "        self.df = df\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "        self.file = file\n",
    "        super().__init__()\n",
    "        \n",
    "    def run(self):\n",
    "                \n",
    "        # === reshaping df for timestap & adjusted headers === #\n",
    "        self.df = reshape_df(self.df,self.file)\n",
    "        \n",
    "        if self.file == 'Irradiance':\n",
    "            \n",
    "            # === Set Column Names === #\n",
    "            self.df.columns = ['GlobalIR','DirectIR','DiffuseIR','WindSpeed','Temperature']\n",
    "            \n",
    "            # === Removing Misread Vemps === #\n",
    "            self.df = clean_irradiance_values(self.df)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # === Set Column Names === #\n",
    "            self.df.columns = ['MonoSi_Vin','MonoSi_Iin','MonoSi_Vout','MonoSi_Iout','PolySi_Vin','PolySi_Iin','PolySi_Vout','PolySi_Iout','TFSi_a_Vin','TFSi_a_Iin','TFSi_a_Vout','TFSi_a_Iout','TFcigs_Vin','TFcigs_Iin','TFcigs_Vout','TFcigs_Iout','TempF_Mono','TempF_Poly','TempF_Amor','TempF_Cigs']\n",
    "        \n",
    "            # === Removing Misread Values === #\n",
    "            self.df = clean_deger_fixed_values(self.df)\n",
    "            \n",
    "        # === resample df to 20s frequency === #\n",
    "        self.df = resample_df(self.df)\n",
    "        \n",
    "        # === Fill hourly gaps with Meteo Data === #\n",
    "        self.df = fill_hour(self.df)\n",
    "        \n",
    "        # === Using PvLib to remove nightime values === #\n",
    "#         self.df = remove_night(self.df)\n",
    "            \n",
    "        print(f\"Imputing {round(self.df.isna().sum().sum()/self.df.size*100,3)}% of the data for {self.month}, {self.year}.\")\n",
    "\n",
    "        self.df = df_imputer(self.df)\n",
    "                \n",
    "        if self.df.isna().any().any():   \n",
    "            raise Exception(f\"The File {self.file}, {self.month} {self.year} still has NaN values\")\n",
    "            \n",
    "        cwd = re.sub(\"Notebooks|Python Scripts\",\"Data/\",os.getcwd())\n",
    "        datapath = cwd + self.year + '/' + self.file + '/'\n",
    "        file = self.month.lower() + '.csv'\n",
    "        self.df.to_csv(datapath + \"/clean_\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec87484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Worker(threading.Thread):\n",
    "    \n",
    "    \"\"\"\n",
    "    Thread worker class for parallel processing.\n",
    "    \n",
    "    Attributes:\n",
    "        queue (Queue): Queue containing file paths.\n",
    "        file (str): File type.\n",
    "        lock (threading.Lock): Lock for thread synchronization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, queue, file, lock):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.file = file\n",
    "        self.lock = lock\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                file_path = self.queue.get(timeout=3) # retrieve file path from the queue\n",
    "            except queue.Empty:\n",
    "                return # If the queue is empty, exit the thread\n",
    "            \n",
    "            data = re.search(r\"/(\\d{4})/[a-zA-Z]*/([a-zA-Z]*)\\.csv\",file_path).group(1,2)\n",
    "            df = pd.read_csv(file_path, sep=\"\\t|,\", engine='python')\n",
    "            self.lock.acquire()\n",
    "            print('Starting',data[1], data[0])\n",
    "            self.lock.release()\n",
    "            Imputer(df, data[1], data[0], self.file).run()\n",
    "            self.lock.acquire()\n",
    "            print('Completed',data[1], data[0])\n",
    "            self.lock.release()\n",
    "            self.queue.task_done() # Notify the queue that the task is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b4221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File (opt: Irradiance/Deger/Fixed): Irradiance\n",
      "Starting november 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pv-solar/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'Wind Speed (m/s) (10 m)'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pv-solar/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/pl/3z04c3v55nz5t091_s4wsj1m0000gn/T/ipykernel_50363/297414507.py\", line 30, in run\n",
      "  File \"/var/folders/pl/3z04c3v55nz5t091_s4wsj1m0000gn/T/ipykernel_50363/1045893949.py\", line 48, in run\n",
      "  File \"/var/folders/pl/3z04c3v55nz5t091_s4wsj1m0000gn/T/ipykernel_50363/3994782445.py\", line 21, in fill_hour\n",
      "  File \"/opt/anaconda3/envs/pv-solar/lib/python3.10/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/opt/anaconda3/envs/pv-solar/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'Wind Speed (m/s) (10 m)'\n"
     ]
    }
   ],
   "source": [
    "starttime = pd.Timestamp.now()\n",
    "        \n",
    "q = queue.Queue()\n",
    "\n",
    "file = input(\"File (opt: Irradiance/Deger/Fixed): \")\n",
    "             \n",
    "file_paths, _ = get_file_paths(file) \n",
    "\n",
    "for file_path in file_paths:\n",
    "#     q.put_nowait(file_path)\n",
    "    # = run for specific month and year, or for test file = #\n",
    "    if re.search(r'nov',file_path.lower()) and re.search(r'2022',file_path): \n",
    "        q.put_nowait(file_path)\n",
    "    \n",
    "lock = threading.Lock()\n",
    "\n",
    "num_jobs = 8\n",
    "\n",
    "for _ in range(num_jobs): \n",
    "    t = Worker(q, file, lock)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "q.join() \n",
    "\n",
    "endtime = pd.Timestamp.now()\n",
    "\n",
    "runtime = endtime - starttime\n",
    "\n",
    "print(\"Start:\",starttime,\"\\nEnd:\",endtime,\"\\nRun Time:\",runtime.total_seconds())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
