{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pvlib\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "module_path = re.sub(r'Notebooks','Python Scripts',os.getcwd())\n",
    "sys.path.append(module_path)\n",
    "from performance_helper import *\n",
    "from ml_helper import *\n",
    "\n",
    "# from warnings import simplefilter\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklego.preprocessing import RepeatingBasisFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neptune\n",
    "import neptune.integrations.sklearn as npt_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapath = re.sub(\"Notebooks\",\"Support Files/\",os.getcwd())\n",
    "ml_df = pd.read_csv(datapath + 'ml_features.csv',index_col=0)\n",
    "ml_df = reshape_ml(test_df, ml_df)\n",
    "\n",
    "# = if reshaping = #\n",
    "ml_df = resample_ml(test_df, ml_df, freq = '20s')\n",
    "ml_df = interpolate(ml_df)\n",
    "ml_df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in ml_df.index]\n",
    "ml_df['Day'] = [d.day for d in ml_df.index]\n",
    "\n",
    "imputation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "\n",
    "    if col != 'Temperature': continue\n",
    "\n",
    "    test_ml_df = ml_df.copy()\n",
    "\n",
    "    test_ml_df[col] = df[df.index.isin(ml_df.index)][col]\n",
    "\n",
    "    X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "    y = test_ml_df[col].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    lasso = Lasso(alpha=0.1) # range of lasso (0.001 to 10)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    selected_indices = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "    if not selected_indices.any(): \n",
    "        (print(f'No features meet current criteria for: {col}'))\n",
    "        continue\n",
    "\n",
    "    print(\"Coefficients:\", list(zip(lasso.coef_,test_ml_df.drop([col], axis = 1).columns)))\n",
    "\n",
    "    X_train_selected = X_train[:, selected_indices]\n",
    "    X_test_selected = X_test[:, selected_indices]\n",
    "\n",
    "    parameters = {\n",
    "        \"hidden_layer_sizes\": (100,100),\n",
    "        \"activation\": \"tanh\",\n",
    "        \"solver\": \"adam\",\n",
    "        \"learning_rate_init\": 0.001,\n",
    "        \"max_iter\": 300,\n",
    "        \"alpha\": 0.0001,\n",
    "        \"beta_1\": 0.9,\n",
    "        \"beta_2\": 0.999,\n",
    "        \"epsilon\": 1e-8\n",
    "    }\n",
    "\n",
    "    # == Model == #\n",
    "\n",
    "    mlp = MLPRegressor(**parameters)\n",
    "\n",
    "    mlp.fit(X_train_selected, y_train)\n",
    "\n",
    "    # == Model == #\n",
    "    \n",
    "    # == Neptune == #\n",
    "\n",
    "#     run = neptune.init_run(\n",
    "#         project=\"ethanmasters/PV-Solar-MLP\",\n",
    "#         api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "#         name=\"MLP-DiffuseIR\",\n",
    "#         tags=[\"MLPRegressor\", \"regression\", \"Temperature\"],\n",
    "#     )\n",
    "\n",
    "#     run[\"parameters\"] = parameters\n",
    "\n",
    "#     run[\"mlp_summary\"] = npt_utils.create_regressor_summary(mlp, X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "#     run.stop()\n",
    "\n",
    "    # == Model == #\n",
    "\n",
    "    print(\"Selected features: \", list(test_ml_df.drop([col], axis = 1).columns))\n",
    "    print(\"Target: \", col)\n",
    "    print(\"Number of layers: \", mlp.n_layers_)\n",
    "    print(\"Number of outputs: \", mlp.n_outputs_)\n",
    "    print(\"Output activation: \", mlp.out_activation_)\n",
    "    print(\"Number of iterations:\", mlp.n_iter_)\n",
    "    print(\"Best loss: \", mlp.best_loss_)\n",
    "    print(\"Current loss: \", mlp.loss_)\n",
    "    print(\"Number of training samples seen: \", mlp.t_)\n",
    "\n",
    "    print(\"\\nLoss Curve: \")\n",
    "\n",
    "    px.line(x = range(mlp.n_iter_), y = mlp.loss_curve_, title = \"Loss Curve\").show()\n",
    "\n",
    "    score = mlp.score(X_test_selected, y_test)\n",
    "    print(\"\\nTest score:\", score)\n",
    "\n",
    "    if test_gaps:\n",
    "        imputation_values = scaler.fit_transform(test_ml_df.drop([col], axis = 1).iloc[test_gaps[col],:].to_numpy()[:, selected_indices])\n",
    "        prediction = mlp.predict(imputation_values)\n",
    "\n",
    "        print(prediction)\n",
    "\n",
    "        df.iloc[test_gaps[col],:][col] = prediction\n",
    "\n",
    "    col_indx += 1\n",
    "\n",
    "    print(\"\\nPerumation Importance: \")\n",
    "\n",
    "    feature_selection(test_ml_df.drop([col], axis = 1).iloc[:, selected_indices], mlp, X_train_selected, y_train)\n",
    "\n",
    "    multilinear_feature_selection(test_ml_df.drop([col], axis = 1).iloc[:, selected_indices], X_test_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# == All Data == #\n",
    "datapath = re.sub(r'Notebooks|Python Scripts','Support Files',os.getcwd())\n",
    "\n",
    "# == Load Irradiance Data == #\n",
    "target_df = pd.read_csv(datapath + '/Irradiance.csv',index_col=0)\n",
    "target_df.index = pd.to_datetime(target_df.index)\n",
    "\n",
    "# == Load ML Data == #\n",
    "training_df = pd.read_csv(datapath + '/meteo_data_cleaned.csv',index_col=0)\n",
    "training_df.index = pd.to_datetime(training_df.index)\n",
    "\n",
    "target_df = target_df[(target_df.index.year != 2022) & (target_df.index.month != 7)]\n",
    "training_df = training_df[(training_df.index.year != 2022) & (training_df.index.month != 7)]\n",
    "training_df = training_df[training_df.index.isin(target_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(training_df)\n",
    "print(target_df)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in target_df.columns:\n",
    "    \n",
    "    if col != 'DirectIR':\n",
    "        continue\n",
    "    \n",
    "    df = target_df[[col]].dropna()\n",
    "\n",
    "    test_ml_df = training_df[training_df.index.isin(df.index)].copy()\n",
    "    \n",
    "    test_ml_df[col] = df[df.index.isin(test_ml_df.index)][col]\n",
    "    \n",
    "    test_ml_df = shuffle(test_ml_df)\n",
    "    \n",
    "    test_ml_df = test_ml_df.drop(['Wind Direction (째) (10 m)'],axis=1)\n",
    "\n",
    "    X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "    y = test_ml_df[col].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # == Activation Function == #\n",
    "#     activation = activations.relu\n",
    "#     activation = activations.tanh\n",
    "#     activation = activations.selu\n",
    "#     activation = activations.elu\n",
    "#     activation = activations.sigmoid\n",
    "#     activation = activations.softmax\n",
    "#     activation = activations.softplus\n",
    "#     activation = activations.softsign\n",
    "    # == Activation Function == #\n",
    "    \n",
    "    # == Optomization Parameters = #\n",
    "    hidden_layer_sizes = (20,10)\n",
    "    learning_rate_ = 0.001\n",
    "    alpha = 0.0001\n",
    "    beta_1_ = 0.9\n",
    "    beta_2_ = 0.999\n",
    "    epsilon_ = 1e-7\n",
    "    # == Optomization Parameters = #\n",
    "    \n",
    "    # == Solver == #\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, beta_1 = beta_1_, beta_2= beta_2_,\n",
    "#                                                     epsilon = epsilon_, amsgrad = True, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate_, name = 'AdamW')\n",
    "#     solver = tf.keras.optimizers.Adamax()\n",
    "#     solver = tf.keras.optimizers.SGD() # horrible\n",
    "    solver = tf.keras.optimizers.RMSprop()\n",
    "    # == Solver == #\n",
    "    \n",
    "    # == Model == #\n",
    "    DirectIR_model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape = [len(test_ml_df.drop([col], axis = 1).columns),],\n",
    "                           name='DirectIR-MLP'),\n",
    "        \n",
    "#         keras.optimizers.Nadam(),\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-1'),\n",
    "        \n",
    "        keras.layers.Dense(hidden_layer_sizes[0],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-1'),\n",
    "        \n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-2'),\n",
    "#         keras.optimizers.Nadam(),\n",
    "            \n",
    "        keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-2'),\n",
    "        \n",
    "        keras.layers.Dense(1, activation='linear', name = \"Output-Layer\")\n",
    "    ])\n",
    "    DirectIR_model.compile(optimizer=solver,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_squared_error'])\n",
    "    # == Model == #\n",
    "    \n",
    "    # == callbacks == #\n",
    "    overfitting = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, restore_best_weights=True)\n",
    "    learning_rate = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "    logdir = \"logs/fit/\" + pd.Timestamp.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "    tb = TensorBoard(log_dir = logdir, histogram_freq = 1)\n",
    "    # == callbacks == #\n",
    "    \n",
    "    # == Fit == #\n",
    "    DirectIR_history = DirectIR_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 25,\n",
    "#                         batch_size = 850,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks = [overfitting,\n",
    "                                     tb]\n",
    "                       )\n",
    "    # == Fit == #\n",
    "    \n",
    "    # == Neptune Run == #\n",
    "#     run = neptune.init_run(\n",
    "#         project=\"ethanmasters/PV-Solar-MLP\",\n",
    "#         api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "#         name=\"MLP-DiffuseIR\",\n",
    "#         tags=[\"MLPRegressor\", \"regression\", \"DirectIR\"],\n",
    "#         )\n",
    "\n",
    "#     DirectIR_history = DirectIR_model.fit(X_train, \n",
    "#                         y_train, \n",
    "#                         epochs = 50, \n",
    "# #                         batch_size = 50,\n",
    "#                         validation_data = (X_test, y_test),\n",
    "#                         callbacks = [NeptuneCallback(run = run, log_model_diagram = True),\n",
    "#                                     overfitting,\n",
    "#                                     tb]\n",
    "#                        )\n",
    "    # == Neptune Run == #\n",
    "    \n",
    "    # == serialize model == #\n",
    "    DirectIR_model.save(f\"{col}_model\")\n",
    "\n",
    "\n",
    "    DirectIR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "for col in target_df.columns:\n",
    "    \n",
    "    if col != 'Temperature':\n",
    "        continue\n",
    "    \n",
    "    df = target_df[[col]].dropna()\n",
    "\n",
    "    test_ml_df = training_df[training_df.index.isin(df.index)].copy()\n",
    "    \n",
    "    test_ml_df[col] = df[df.index.isin(test_ml_df.index)][col]\n",
    "    \n",
    "    test_ml_df = shuffle(test_ml_df)\n",
    "    \n",
    "    test_ml_df = test_ml_df.drop(['Wind Speed (km/h) (10 m)'],axis=1)\n",
    "\n",
    "    X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "    y = test_ml_df[col].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # == Activation Function == #\n",
    "#     activation = activations.relu\n",
    "#     activation = activations.tanh\n",
    "#     activation = activations.selu\n",
    "#     activation = activations.elu\n",
    "#     activation = activations.sigmoid\n",
    "#     activation = activations.softmax\n",
    "#     activation = activations.softplus\n",
    "#     activation = activations.softsign\n",
    "    # == Activation Function == #\n",
    "    \n",
    "    # == Optomization Parameters = #\n",
    "    hidden_layer_sizes = (50,25,25)\n",
    "    learning_rate_ = 0.001\n",
    "    alpha = 0.0001\n",
    "    beta_1_ = 0.9\n",
    "    beta_2_ = 0.999\n",
    "    epsilon_ = 1e-7\n",
    "    # == Optomization Parameters = #\n",
    "    \n",
    "    # == Solver == #\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, beta_1 = beta_1_, beta_2= beta_2_,\n",
    "#                                                     epsilon = epsilon_, amsgrad = True, name = \"Adam\")\n",
    "    solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate_, name = 'AdamW')\n",
    "#     solver = tf.keras.optimizers.Adamax()\n",
    "#     solver = tf.keras.optimizers.SGD() # horrible\n",
    "#     solver = tf.keras.optimizers.RMSprop()\n",
    "    # == Solver == #\n",
    "    \n",
    "    # == Model == #\n",
    "    Temperature_model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape = [len(test_ml_df.drop([col], axis = 1).columns),],\n",
    "                           name='Temperature-MLP'),\n",
    "        \n",
    "#         keras.optimizers.Nadam(),\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-1'),\n",
    "        \n",
    "        keras.layers.Dense(hidden_layer_sizes[0],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-1'),\n",
    "        \n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-2'),\n",
    "#         keras.optimizers.Nadam(),\n",
    "            \n",
    "        keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-2'),\n",
    "        \n",
    "        keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-3'),\n",
    "        \n",
    "        keras.layers.Dense(1, activation='linear', name = \"Output-Layer\")\n",
    "    ])\n",
    "    Temperature_model.compile(optimizer=solver,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_squared_error'])\n",
    "    # == Model == #\n",
    "    \n",
    "    # == callbacks == #\n",
    "    overfitting = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, restore_best_weights=True)\n",
    "    learning_rate = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "    logdir = \"logs/fit/\" + pd.Timestamp.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tb = TensorBoard(log_dir = logdir, histogram_freq = 1)\n",
    "    # == callbacks == #\n",
    "    \n",
    "    # == Fit == #\n",
    "    Temperature_history = Temperature_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 25,\n",
    "#                         batch_size = 850,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks = [overfitting,\n",
    "                                     tb]\n",
    "                       )\n",
    "    # == Fit == #\n",
    "    \n",
    "    # == Neptune Fit == #\n",
    "#     run = neptune.init_run(\n",
    "#         project=\"ethanmasters/PV-Solar-MLP\",\n",
    "#         api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "#         name=\"MLP-DiffuseIR\",\n",
    "#         tags=[\"MLPRegressor\", \"regression\", \"Temperature\"],\n",
    "#         )\n",
    "#     Temperature_history = Temperature_model.fit(X_train, \n",
    "#                         y_train, \n",
    "#                         epochs = 25, \n",
    "# #                         batch_size = 50,\n",
    "#                         validation_data = (X_test, y_test),\n",
    "#                         callbacks = [NeptuneCallback(run = run, log_model_diagram = True),\n",
    "#                                     overfitting,\n",
    "#                                     tb]\n",
    "#                        )\n",
    "    # == Neptune Fit == #\n",
    "    \n",
    "    # == serialize model == #\n",
    "    Temperature_model.save(f\"{col}_model\")\n",
    "\n",
    "    Temperature_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# == WindSpeed == #\n",
    "\n",
    "for col in target_df.columns:\n",
    "    \n",
    "    if col != 'WindSpeed':\n",
    "        continue\n",
    "    \n",
    "    df = target_df[[col]].dropna()\n",
    "\n",
    "    test_ml_df = training_df[training_df.index.isin(df.index)].copy()\n",
    "    \n",
    "    test_ml_df[col] = df[df.index.isin(test_ml_df.index)][col]\n",
    "    \n",
    "    test_ml_df = shuffle(test_ml_df)\n",
    "    \n",
    "    test_ml_df = test_ml_df.drop(['Temperature (째C) (2 m elevation corrected)', 'Wind Direction (째) (10 m)',\n",
    "       'Cloud Cover Total (%) (sfc)'],axis=1)\n",
    "\n",
    "    X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "    y = test_ml_df[col].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # == Activation Function == #\n",
    "#     activation = activations.relu\n",
    "#     activation = activations.tanh\n",
    "#     activation = activations.selu\n",
    "#     activation = activations.elu\n",
    "#     activation = activations.sigmoid\n",
    "#     activation = activations.softmax\n",
    "#     activation = activations.softplus\n",
    "#     activation = activations.softsign\n",
    "# == Activation Function == #\n",
    "    \n",
    "    # == Optomization Parameters = #\n",
    "    hidden_layer_sizes = (20,10)\n",
    "    learning_rate_ = 0.001\n",
    "    alpha = 0.0001\n",
    "    beta_1_ = 0.9\n",
    "    beta_2_ = 0.999\n",
    "    epsilon_ = 1e-7\n",
    "    # == Optomization Parameters = #\n",
    "    \n",
    "    \n",
    "    # == Solver == #\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, beta_1 = beta_1_, beta_2= beta_2_,\n",
    "#                                                     epsilon = epsilon_, amsgrad = True, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate_, name = 'AdamW')\n",
    "#     solver = tf.keras.optimizers.Adamax()\n",
    "#     solver = tf.keras.optimizers.SGD() # horrible\n",
    "    solver = tf.keras.optimizers.RMSprop()\n",
    "    # == Solver == #\n",
    "    \n",
    "    # == Model == #\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape = [len(test_ml_df.drop([col], axis = 1).columns),],\n",
    "                           name='WindSpeed-MLP'),\n",
    "        \n",
    "#         keras.optimizers.Nadam(),\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-1'),\n",
    "        \n",
    "        keras.layers.Dense(hidden_layer_sizes[0],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-1'),\n",
    "        \n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-2'),\n",
    "#         keras.optimizers.Nadam(),\n",
    "            \n",
    "        keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-2'),\n",
    "        \n",
    "        keras.layers.Dense(1, activation='linear', name = \"Output-Layer\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=solver,\n",
    "                      loss='mean_absolute_error',\n",
    "                      metrics=['mean_absolute_error'])\n",
    "    # == Model == #\n",
    "    \n",
    "    # == callbacks == #\n",
    "    overfitting = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, restore_best_weights=True)\n",
    "    learning_rate = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "    logdir = \"logs/fit/\" + pd.Timestamp.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tb = TensorBoard(log_dir = logdir, histogram_freq = 1)\n",
    "    # == callbacks == #\n",
    "    \n",
    "    # == Local Run == #\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 25,\n",
    "#                         batch_size = 850,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks = [overfitting,\n",
    "                                     tb]\n",
    "                       )\n",
    "    # == Local Run == #\n",
    "    \n",
    "    # == Neptune Run == # \n",
    "#     run = neptune.init_run(\n",
    "#         project=\"ethanmasters/PV-Solar-MLP\",\n",
    "#         api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "#         name=\"MLP-DiffuseIR\",\n",
    "#         tags=[\"MLPRegressor\", \"regression\", \"WindSpeed\"],\n",
    "#         )\n",
    "\n",
    "#     history = model.fit(X_train, \n",
    "#                         y_train, \n",
    "#                         epochs = 10, \n",
    "# #                         batch_size = 50,\n",
    "#                         validation_data = (X_test, y_test),\n",
    "#                         callbacks = [NeptuneCallback(run = run, log_model_diagram = True),\n",
    "#                                     overfitting,\n",
    "#                                     tb]\n",
    "#                        )\n",
    "    # == Neptune Run == # \n",
    "    \n",
    "    # == serialize model == #\n",
    "    model.save(f\"{col}_model\")\n",
    "    # == serialize model == #\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======= Diffuse Model ======= #\n",
    "\n",
    "for col in target_df.columns:\n",
    "    \n",
    "    if col != 'DiffuseIR':\n",
    "        continue\n",
    "    \n",
    "    df = target_df[[col]].dropna()\n",
    "\n",
    "    test_ml_df = training_df[training_df.index.isin(df.index)].copy()\n",
    "    \n",
    "    test_ml_df[col] = df[df.index.isin(test_ml_df.index)][col]\n",
    "    \n",
    "    test_ml_df = shuffle(test_ml_df)\n",
    "    \n",
    "    test_ml_df = test_ml_df.drop(['Wind Direction (째) (10 m)'],axis=1)\n",
    "\n",
    "    X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "    y = test_ml_df[col].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # == Activation Function == #\n",
    "#     activation = activations.relu\n",
    "#     activation = activations.tanh\n",
    "#     activation = activations.selu\n",
    "#     activation = activations.elu\n",
    "#     activation = activations.sigmoid\n",
    "#     activation = activations.softmax\n",
    "#     activation = activations.softplus\n",
    "#     activation = activations.softsign\n",
    "    # == Activation Function == #\n",
    "    \n",
    "    # == Optomization Parameters = #\n",
    "    hidden_layer_sizes = (20,10)\n",
    "    learning_rate_ = 0.001\n",
    "    alpha = 0.0001\n",
    "    beta_1_ = 0.9\n",
    "    beta_2_ = 0.999\n",
    "    epsilon_ = 1e-7\n",
    "    # == Optomization Parameters = #\n",
    "    \n",
    "    # == Solver == #\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, beta_1 = beta_1_, beta_2= beta_2_,\n",
    "#                                                     epsilon = epsilon_, amsgrad = True, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate_, name = 'AdamW')\n",
    "#     solver = tf.keras.optimizers.Adamax()\n",
    "#     solver = tf.keras.optimizers.SGD() # horrible\n",
    "    solver = tf.keras.optimizers.RMSprop()\n",
    "    # == Solver == #\n",
    "    \n",
    "    # == Model == #\n",
    "    DiffuseIR_model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape = [len(test_ml_df.drop([col], axis = 1).columns),],\n",
    "                           name='DiffuseIR-MLP'),\n",
    "        \n",
    "#         keras.optimizers.Nadam(),\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-1'),\n",
    "        \n",
    "        keras.layers.Dense(hidden_layer_sizes[0],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-1'),\n",
    "        \n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-2'),\n",
    "#         keras.optimizers.Nadam(),\n",
    "            \n",
    "        keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                           kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                           activation = 'relu',\n",
    "                           name = 'Hidden-Layer-2'),\n",
    "        \n",
    "        keras.layers.Dense(1, activation='linear', name = \"Output-Layer\")\n",
    "    ])\n",
    "\n",
    "    DiffuseIR_model.compile(optimizer=solver,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_squared_error'])\n",
    "    # == Model == #\n",
    "    \n",
    "    # == callbacks == #\n",
    "    overfitting = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, restore_best_weights=True)\n",
    "    learning_rate = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "    logdir = \"logs/fit/\" + pd.Timestamp.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tb = TensorBoard(log_dir = logdir, histogram_freq = 1)\n",
    "    # == callbacks == #\n",
    "    \n",
    "    # == Local Run == #\n",
    "    DiffuseIR_history = DiffuseIR_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 25,\n",
    "#                         batch_size = 850,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        callbacks = [overfitting,\n",
    "                                     tb]\n",
    "                       )\n",
    "    # == Local Run == #\n",
    "    \n",
    "    # == Neptune Run == # \n",
    "#     run = neptune.init_run(\n",
    "#         project=\"ethanmasters/PV-Solar-MLP\",\n",
    "#         api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "#         name=\"MLP-DiffuseIR\",\n",
    "#         tags=[\"MLPRegressor\", \"regression\", \"DiffuseIR\"],\n",
    "#         )\n",
    "\n",
    "#     DiffuseIR_history = DiffuseIR_model.fit(X_train, \n",
    "#                         y_train, \n",
    "#                         epochs = 10, \n",
    "# #                         batch_size = 50,\n",
    "#                         validation_data = (X_test, y_test),\n",
    "#                         callbacks = [NeptuneCallback(run = run, log_model_diagram = True),\n",
    "#                                     overfitting,\n",
    "#                                     tb]\n",
    "#                        )\n",
    "    # == Neptune Run == # \n",
    "    \n",
    "    # == serialize model == #\n",
    "    DiffuseIR_model.save(f\"{col}_model\")\n",
    "    # == serialize model == #\n",
    "\n",
    "    DiffuseIR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "starttime = pd.Timestamp.now()\n",
    "# ======= Any Model ======= #\n",
    "\n",
    "df = target_df.dropna()\n",
    "\n",
    "test_ml_df = training_df[training_df.index.isin(df.index)].copy()\n",
    "\n",
    "test_ml_df = pd.concat([test_ml_df,df],axis=1,ignore_index=False)\n",
    "\n",
    "test_ml_df = shuffle(test_ml_df)\n",
    "\n",
    "X = test_ml_df.drop(df.columns, axis = 1).to_numpy()\n",
    "y = test_ml_df.drop(training_df.columns, axis = 1).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# == Activation Function == #\n",
    "#     activation = activations.relu\n",
    "#     activation = activations.tanh\n",
    "#     activation = activations.selu\n",
    "#     activation = activations.elu\n",
    "#     activation = activations.sigmoid\n",
    "#     activation = activations.softmax\n",
    "#     activation = activations.softplus\n",
    "#     activation = activations.softsign\n",
    "\n",
    "# == Optomization Parameters = #\n",
    "hidden_layer_sizes = (300,200,150)\n",
    "learning_rate_ = 0.0001\n",
    "alpha = 0.0001\n",
    "beta_1_ = 0.9\n",
    "beta_2_ = 0.999\n",
    "epsilon_ = 1e-7\n",
    "# == Optomization Parameters = #\n",
    "\n",
    "# == Solver == #\n",
    "#     solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_, beta_1 = beta_1_, beta_2= beta_2_,\n",
    "#                                                     epsilon = epsilon_, amsgrad = True, name = \"Adam\")\n",
    "solver = tf.keras.optimizers.Adam(learning_rate=learning_rate_,name = \"Adam\")\n",
    "#     solver = tf.keras.optimizers.experimental.AdamW(learning_rate=learning_rate_, name = 'AdamW')\n",
    "#     solver = tf.keras.optimizers.Adamax()\n",
    "#     solver = tf.keras.optimizers.SGD() # horrible\n",
    "# solver = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "x_shape, y_shape = training_df.shape\n",
    "# == Solver == #\n",
    "\n",
    "# == Model == #\n",
    "multi_model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape = (y_shape,), name='Temperature-MLP'),\n",
    "\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-1'),\n",
    "\n",
    "    keras.layers.Dense(hidden_layer_sizes[0],\n",
    "                       kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                       activation = 'relu',\n",
    "                       name = 'Hidden-Layer-1'),\n",
    "\n",
    "#         keras.layers.ReLU(name = 'Activation-Layer-2'),\n",
    "#         keras.optimizers.Nadam(),\n",
    "\n",
    "    keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                       kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                       activation = 'relu',\n",
    "                       name = 'Hidden-Layer-2'),\n",
    "    \n",
    "    keras.layers.Dense(hidden_layer_sizes[1],\n",
    "                       kernel_initializer=tf.keras.initializers.HeNormal(),\n",
    "                       activation = 'relu',\n",
    "                       name = 'Hidden-Layer-3'),\n",
    "\n",
    "    keras.layers.Dense(len(df.columns), name = \"Output-Layer\")\n",
    "])\n",
    "\n",
    "multi_model.compile(optimizer=solver,\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "# == Model == #\n",
    "\n",
    "# == callbacks == #\n",
    "overfitting = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 20)\n",
    "learning_rate = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 20))\n",
    "logdir = \"logs/fit/\" + pd.Timestamp.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb = TensorBoard(log_dir = logdir, histogram_freq = 1)\n",
    "# == callbacks == #\n",
    "\n",
    "# == Local Fit == #\n",
    "# multi_history = multi_model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs = 10,\n",
    "# #                     batch_size = 3000,\n",
    "#                     validation_data = (X_test, y_test),\n",
    "#                     callbacks = [overfitting,\n",
    "#                                  tb]\n",
    "#                    )\n",
    "\n",
    "# == Neptune Fit == #\n",
    "run = neptune.init_run(\n",
    "    project=\"ethanmasters/PV-Solar-MLP\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "    name=\"MLP-DiffuseIR\",\n",
    "    tags=[\"MLPRegressor\", \"regression\", \"MultiOutput\"],\n",
    "    )\n",
    "multi_history = multi_model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs = 100, \n",
    "#                         batch_size = 50,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    callbacks = [NeptuneCallback(run = run, log_model_diagram = True),\n",
    "                                overfitting,\n",
    "                                tb]\n",
    "                   )\n",
    "# == Neptune Fit == #\n",
    "\n",
    "# == serialize model == #\n",
    "multi_model.save(f\"multivariate_mlp_model\")\n",
    "\n",
    "multi_model.summary()\n",
    "\n",
    "endtime = pd.Timestamp.now()\n",
    "runtime = endtime - starttime\n",
    "print(\"Run Time:\", runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = multi_history.history\n",
    "model = multi_model1\n",
    "# col = \"Temperature\"\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(score)\n",
    "\n",
    "model_df = pd.DataFrame(history)\n",
    "mse_df = model_df[['mean_absolute_error','val_mean_absolute_error']]\n",
    "mse_df.plot(figsize=(12,6))\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "\n",
    "loss_df = model_df[['loss','val_loss']]\n",
    "loss_df.plot(figsize=(12,6))\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1) # set the vertical range to [0-1]\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (18, 8)\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False \n",
    "\n",
    "plt.plot(\n",
    "    np.arange(1, 26), \n",
    "    history['loss'], \n",
    "    label='Loss', lw=3\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, 11), \n",
    "    history['val_loss'], \n",
    "    label='Value Loss', lw=3\n",
    ")\n",
    "# plt.plot(\n",
    "#     np.arange(1, 21), \n",
    "#     history.history['lr'], \n",
    "#     label='Learning rate', color='#000', lw=3, linestyle='--'\n",
    "# )\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(1, 11), \n",
    "    history['mean_squared_error'], \n",
    "    label='MSE', lw=3\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, 11), \n",
    "    history['val_mean_squared_error'], \n",
    "    label='Value MSE', lw=3\n",
    ")\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dalex as dx\n",
    "X, y = pd.DataFrame(X_test, columns = test_ml_df.columns), y_test\n",
    "exp = dx.Explainer(model, X, y, label=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.model_parts().plot()\n",
    "run[\"model/performance/model_parts\"].upload(exp.model_parts().plot(show=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.model_profile().plot()\n",
    "run[\"model/performance/model_profile\"].upload(exp.model_profile().plot(show=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.model_diagnostics().plot()\n",
    "run[\"model/performance/model_diagnostics\"].upload(exp.model_diagnostics().plot(show=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.model_diagnostics().result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# surrogate_model = explainer.model_surrogate(max_vars=4, max_depth=3)\n",
    "surrogate_model = exp.model_surrogate()\n",
    "surrogate_model.performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "surrogate_model.plot()\n",
    "# run[\"model/performance/surrogate_model\"].upload(surrogate_model.plot(show=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
