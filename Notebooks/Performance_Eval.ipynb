{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pvlib\n",
    "import math\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pykalman import KalmanFilter\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Kalman Smoothing using R objects\n",
    "import rpy2.robjects as robjects\n",
    "# import R packages\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Impute TS\n",
    "imputeTS = importr('imputeTS') \n",
    "kalman_StructTs = robjects.r['na.kalman']\n",
    "kalman_auto_arima = robjects.r['na.kalman']\n",
    "sea_decom = robjects.r['na.seadec']\n",
    "sea_split = robjects.r['na.seasplit']\n",
    "\n",
    "# # Zoo\n",
    "# zoo = importr('zoo') \n",
    "# kalman_StructTs_zoo = robjects.r['na.StructTS']\n",
    "# nan_interpolation_sea = robjects.r['na.approx']\n",
    "\n",
    "# # Forecast\n",
    "# forecast = importr('forecast') \n",
    "# nan_interpolation_forecast = robjects.r['na.interp']\n",
    "\n",
    "# import rpy2.robjects as ro\n",
    "# from rpy2.robjects import pandas2ri\n",
    "# pandas2ri.activate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(file):\n",
    "    path_list = []\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Data/',os.getcwd())\n",
    "    for dir in os.scandir(datapath):\n",
    "        if re.search(r'\\.',dir.name): continue\n",
    "        year_path = datapath + f\"{dir.name}\"\n",
    "        for dir in os.scandir(year_path):\n",
    "            if dir.name == file:\n",
    "                month_path = year_path + f\"/{dir.name}/\"\n",
    "                for dir in os.scandir(month_path):\n",
    "                    if not re.search(r'\\.csv|\\.xlsx',dir.name): continue\n",
    "                    path_list += [month_path + f\"{dir.name}\"]\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_func(year,month,year_2,month_2,path_list,file):\n",
    "    path_found = []\n",
    "    for path in path_list:\n",
    "        path_copy = path.lower()\n",
    "        data = re.search(r\"/(\\d{4})/[a-z]*/([a-z]*)\\.\",path_copy).group(1,2)\n",
    "        if year:\n",
    "            if not path_found:\n",
    "                if re.search(fr\"{year}\",data[0]) and re.search(fr\"{month}\",data[1]):\n",
    "                    path_found += [path]\n",
    "            else:\n",
    "                if re.search(fr\"{year}\",data[0]) and re.search(fr\"{month}\",data[1]):\n",
    "                    path_found += [path]\n",
    "                    path_found = [path_found[1],path_found[0]]\n",
    "\n",
    "        if year_2:\n",
    "            if re.search(fr\"{year_2}\",data[0]) and re.search(fr\"{month_2}\",data[1]):\n",
    "                path_found += [path]\n",
    "\n",
    "    return path_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrounding_month(month, year , cond):\n",
    "    month_dict = {0:'dec',13:'jan',8:'aug',9:'sep',10:'oct',11:'nov',12:'dec',1:'jan',2:'feb',3:'mar',4:'apr',5:'may',6:'jun',7:'jul'}\n",
    "    if cond == 'prev':\n",
    "        if month == 1:\n",
    "            year -= 1\n",
    "        return month_dict[month-1], year\n",
    "    else:\n",
    "        if month == 12:\n",
    "            year += 1\n",
    "        return month_dict[month+1], year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_df(df,file):\n",
    "    df['DayID'] = df['DayID'].astype(str)\n",
    "    df['TimeID'] = df['TimeID'].astype(str)\n",
    "    df['date'] = df['DayID'] + 'T' +  df['TimeID']\n",
    "    df = df.drop(columns = ['DayID','TimeID'])\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    df = df.set_index('date')\n",
    "    df.index = df.index.tz_localize(tz = 'Etc/UTC')\n",
    "    df = df.sort_index()\n",
    "    if file == 'Irradiance':\n",
    "        df.columns = ['GlobalIR','DirectIR','DiffuseIR','WindSpeed','Temperature']\n",
    "    else:\n",
    "        df.columns = ['MonoSi_Vin','MonoSi_Iin','MonoSi_Vout','MonoSi_Iout','PolySi_Vin','PolySi_Iin','PolySi_Vout','PolySi_Iout','TFSi_a_Vin','TFSi_a_Iin','TFSi_a_Vout','TFSi_a_Iout','TFcigs_Vin','TFcigs_Iin','TFcigs_Vout','TFcigs_Iout','TempF_Mono','TempF_Poly','TempF_Amor','TempF_Cigs']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_times(df):\n",
    "    \n",
    "    # creating of list of times to find interval gaps\n",
    "    time_list = list(df.index)\n",
    "    \n",
    "    # calculating interval gaps if > 21s and storing [interval length (s), start_time, end_time]\n",
    "    missing_intervals = [[(time_list[time+1] - time_list[time]).total_seconds(),time_list[time],time_list[time+1]]\n",
    "                 for time in range(len(time_list)-1) if (time_list[time+1] - time_list[time]).total_seconds() > 21]\n",
    "    \n",
    "    # generating timestamps to fill interval gaps \n",
    "    interval_list = [element for sublist in [pd.date_range(start=interval[1],\n",
    "                             end=interval[2]-pd.Timedelta(1,'s'),\n",
    "                             freq='11s') for interval in missing_intervals] for element in sublist]\n",
    "    \n",
    "    # checking for missing values at the beginning of the month\n",
    "    if time_list[0] > time_list[0].replace(day=1,hour=1):\n",
    "        print(\"Month found with missing values at the beginning of the month.\")\n",
    "        print('Time:',time_list[0])\n",
    "        interval_list += [time for time in pd.date_range(start=time_list[0].replace(day=1,hour=0,minute=0,second=0),\n",
    "                             end=time_list[0]-pd.Timedelta(1,'s'),\n",
    "                             freq='11s')]\n",
    "        missing_intervals += [[(time_list[0] - time_list[0].replace(day=1,hour=0,minute=0,second=0)).total_seconds(),\n",
    "                             time_list[0].replace(day=1,hour=0,minute=0,second=0),time_list[0]]]\n",
    "        \n",
    "    # checking for missing values at the end of the month    \n",
    "    next_month = time_list[0].replace(day=28,hour=0,minute=0,second=0) + pd.Timedelta(4,'d')\n",
    "    last_day = next_month - pd.Timedelta(next_month.day,'d')\n",
    "    if time_list[-1] < last_day.replace(hour = 23,minute=0):\n",
    "        print(\"Month found with missing values at the end of the month.\")\n",
    "        print('Time:',time_list[-1])\n",
    "        interval_list += [time for time in pd.date_range(start=time_list[-1],\n",
    "                     end=last_day.replace(hour=23,minute=59,second=59),\n",
    "                     freq='11s')]\n",
    "        missing_intervals += [[(last_day.replace(hour=23,minute=59,second=59) - time_list[-1]).total_seconds(),\n",
    "                             time_list[-1],last_day.replace(hour=23,minute=59,second=59)]]\n",
    "        \n",
    "    interval_list = list(set(interval_list))\n",
    "    mt_df = pd.DataFrame(index=interval_list,columns=df.columns)\n",
    "    mt_df.loc[interval_list] = np.nan\n",
    "    df = pd.concat([df,mt_df], axis = 0).sort_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_night(df):\n",
    "    lat = 49.102\n",
    "    lon = 6.215\n",
    "    alt = 220\n",
    "    solpos = pvlib.solarposition.get_solarposition(\n",
    "        time=df.index,latitude=lat,longitude=lon,altitude=alt,method='pyephem')\n",
    "    df = df[solpos['zenith'] <= 90]\n",
    "    return df\n",
    "\n",
    "def irr(df):\n",
    "    # Removing Temperature Values #\n",
    "    df[df['Temperature'] > 60] = np.nan\n",
    "\n",
    "    # Removing Wind Speed Values #\n",
    "    df[df['WindSpeed'] > 100] = np.nan\n",
    "\n",
    "    # Removing DirectIR Values #\n",
    "    df[df['DirectIR'] > 2000] = np.nan\n",
    "\n",
    "    # Removing DiffuseIR Values #\n",
    "    df[df['DiffuseIR'] > 2000] = np.nan\n",
    "\n",
    "    # Removing Negative Values #\n",
    "    df[df < 0] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def deg_fix(df):\n",
    "    # Removing Negative Values #\n",
    "    df[df < 0] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaner(path_list,file):\n",
    "    df_clean = pd.DataFrame()\n",
    "\n",
    "    for path in path_list:\n",
    "\n",
    "        df_load = pd.read_csv(path,sep=\"\\t|,\",engine='python')\n",
    "        \n",
    "        # === In case a file isn't stored properly or empty === #\n",
    "        if df_load.empty:\n",
    "            raise Exception(\"Loaded an empty dataframe\")\n",
    "        \n",
    "        # ==== reshaping df for timestap & adjusted headers ==== #\n",
    "        df_load = reshape_df(df_load)\n",
    "\n",
    "        # === filling gaps in time intervals === #\n",
    "        df_load = add_missing_times(df_load)\n",
    "\n",
    "        # # ==== Using PvLib to remove nightime values === #\n",
    "        df_load = remove_night(df_load)\n",
    "        \n",
    "        if file == 'Irradiance':\n",
    "            # === Removing Values for Irradiance === #\n",
    "            df = irr(df)\n",
    "\n",
    "        else:\n",
    "            # === Removing Values for Deger & Fixed === #\n",
    "            df = deg_fix(df)\n",
    "\n",
    "        df_clean = pd.concat([df_clean,df_load],axis=0,ignore_index=False).sort_index()\n",
    "        \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(pre_df, df_2):\n",
    "    \n",
    "    pre_df = pre_df.dropna(axis=0)\n",
    "    if 'GlobalIR' in pre_df.columns:\n",
    "        pre_df,df_2 = pre_df.drop(['GlobalIR'],axis=1),df_2.drop(['GlobalIR'],axis=1)\n",
    "    copy_df = pre_df.copy()\n",
    "    pre_df, df_2 = pre_df.reset_index(), df_2.reset_index()\n",
    "    pre_df, df_2 = pre_df.drop(['index'],axis=1),df_2.drop(['index'],axis=1)    \n",
    "        \n",
    "    # create a boolean mask that identifies NaN values\n",
    "    nan_mask = df_2.isna() \n",
    "    # use np.where to find integer positions of NaN values\n",
    "    nan_indices = list(np.where(nan_mask))\n",
    "    scaler = MinMaxScaler(feature_range=(pre_df.index[0], pre_df.index[-1]))\n",
    "    nan_indices[0] = scaler.fit_transform(nan_indices[0].reshape(-1, 1))\n",
    "    nan_indices[0] = nan_indices[0].reshape(1,-1)[0].astype(int)\n",
    "    \n",
    "    nan_indices = tuple(zip(nan_indices))\n",
    "        \n",
    "    pre_df.iloc[nan_indices] = np.nan\n",
    "    pre_df.index = copy_df.index    \n",
    "\n",
    "    pre_df.to_csv('Data_Summary_Files/test_data.csv')\n",
    "        \n",
    "    return pre_df, copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_mask(pre_df, date, nan_gap):\n",
    "\n",
    "    if not nan_gap:\n",
    "        nan_gap = float('inf')\n",
    "    else:\n",
    "        if not re.match(r'\\d*',nan_gap):\n",
    "            raise Exception(f\"Incorrect input NaN gap size: {nan_gap}\")\n",
    "        else:\n",
    "            nan_gap = int(nan_gap)\n",
    "    \n",
    "    test_gaps = {}\n",
    "    for col in range(len(pre_df.columns)):\n",
    "        inx_ind = []\n",
    "        test_gaps[pre_df.columns[col]] = []\n",
    "        for index in range(len(pre_df.index)):\n",
    "            if pre_df.index[index].month != int(date): continue\n",
    "            if index in inx_ind: continue\n",
    "            c = 0\n",
    "            while np.isnan(pre_df.iloc[index+c,col]) and pre_df.iloc[index+c].name != pre_df.iloc[-1].name:\n",
    "                inx_ind += [index+c]\n",
    "                c += 1\n",
    "            if not c: continue\n",
    "            dt = (pre_df.index[index+c] - pre_df.index[index]).total_seconds()\n",
    "            if dt <= nan_gap:\n",
    "                test_gaps[pre_df.columns[col]] += inx_ind\n",
    "        test_gaps[pre_df.columns[col]] = list(set(test_gaps[pre_df.columns[col]]))\n",
    "        \n",
    "    return nan_gap, test_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_gaps(df, file):\n",
    "    \n",
    "    month_i = df.index[0].month\n",
    "\n",
    "    if int(df.index[0].month) == 2 and int(df.index[0].year) == 2023:\n",
    "        return False, False, month_i\n",
    "        \n",
    "    beg_ind = False\n",
    "    end_ind = False\n",
    "    path_list = execute(file)\n",
    "    \n",
    "    if df.iloc[0:int(len(df)/df.iloc[-1].name.day*5),:].isna().sum().sum() / df.iloc[0:int(len(df)/df.iloc[-1].name.day*5),:].size*100 > 5:\n",
    "        if int(df.index[0].month) == 1 and int(df.index[0].year) == 2021:\n",
    "            print(\"January 2021 is the first month observed can't join prior month.\")\n",
    "            return False, False, month_i\n",
    "        month,year = surrounding_month(df.iloc[0].name.month,df.iloc[0].name.year ,'prev')\n",
    "        path_list = path_func(year,month,'', '', path_list, file)\n",
    "        load_beg = df_cleaner(path_list,file)\n",
    "        if 'GlobalIR' in load_beg.columns:\n",
    "            load_beg = load_beg.drop(['GlobalIR'],axis=1)\n",
    "        beg_ind = True\n",
    "        \n",
    "    if df.iloc[int(len(df)/df.iloc[-1].name.day*(df.iloc[-1].name.day - 4)):-1, :].isna().sum().sum() / df.iloc[int(len(df)/df.iloc[-1].name.day*(df.iloc[-1].name.day - 4)):-1, :].size*100 > 5:\n",
    "        if int(df.index[0].month) == 2 and int(df.index[0].year) == 2023:\n",
    "            print(\"Feburary 2021 is the last month observed can't join following month.\")\n",
    "            return False, False, month_i\n",
    "        month,year = surrounding_month(df.iloc[0].name.month, df.iloc[0].name.year, '')\n",
    "        path_list = path_func(year, month, '', '', path_list, file)\n",
    "        load_end = df_cleaner(path_list,file)\n",
    "        if 'GlobalIR' in load_end.columns:\n",
    "            load_end = load_end.drop(['GlobalIR'],axis=1)\n",
    "        end_ind = True\n",
    "    \n",
    "    if beg_ind and end_ind:\n",
    "        return load_beg, load_end, month_i\n",
    "    elif beg_ind:\n",
    "        return load_beg, False, month_i\n",
    "    elif end_ind:\n",
    "        return False, load_end, month_i\n",
    "    else:\n",
    "        return False, False, month_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metric(imputed_df, copy_df, test_gaps):\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for col in imputed_df.columns:\n",
    "        \n",
    "        pred_val = imputed_df[col].iloc[test_gaps[col]]\n",
    "        test_val = copy_df[col].iloc[test_gaps[col]]\n",
    "\n",
    "        mae = mean_absolute_error(test_val,pred_val)\n",
    "        mse = mean_squared_error(test_val, pred_val)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(test_val, pred_val)\n",
    "        \n",
    "        error_dict[col] = [mae,mse,rmse,r2]\n",
    "\n",
    "        print(f\"For {col}\")\n",
    "        print(f\"Mean Absolute Error: {mae}\")\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse}\")\n",
    "        print(f\"R2 Score: {r2} \\n\")\n",
    "        \n",
    "    error_df = pd.DataFrame(error_dict).T\n",
    "    error_df.columns = ['mae','mse','rmse','r2']\n",
    "    \n",
    "    print(error_df)\n",
    "    \n",
    "    px.bar(error_df, x = error_df.index, y = error_df['r2']).show()\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(year, month, year_2, month_2, file):\n",
    "    if not re.search(r'\\d{4}',year):\n",
    "        raise Exception(f\"Incorret Input: {year}\")\n",
    "    elif not re.search(r'[A-Za-z]{3}',month):\n",
    "        raise Exception(f\"Incorret Input: {month}\")\n",
    "    elif not re.search(r'[A-Za-z]{3}',month_2):\n",
    "        raise Exception(f\"Incorret Input: {month_2}\")\n",
    "    elif not re.search(r'\\d{4}',year_2):\n",
    "        raise Exception(f\"Incorret Input: {year_2}\")\n",
    "    elif not [file_i for file_i in ['Irradiance','Deger','Fixed'] if re.search(fr'{file}',file_i)]:\n",
    "        raise Exception(f\"Incorret Input: File\")\n",
    "    else:\n",
    "        path_list = execute(file)\n",
    "        if not year and month and year_2 and month_2:\n",
    "            df = df_cleaner(path_list,file)\n",
    "            df_2 = pd.DataFrame()\n",
    "        else:\n",
    "            path_list = path_func(year,month,year_2,month_2,path_list,file)\n",
    "            df = df_cleaner([path_list[0]],file)\n",
    "            df_2 = df_cleaner([path_list[1]],file)\n",
    "    return df, df_2,file, year_2, month_2, year, month\n",
    "\n",
    "# Load all the Data\n",
    "df_load, df2_load, file, year_2, month_2, year, month = main(year = input(\"Year (format: YYYY): \"),month = input(\"Month (format: jul): \"),\n",
    "     year_2 = input(\"Second Year (format: YYYY): \"),month_2 = input(\"Second Month (format: jul): \"),\n",
    "     file = input(\"File (opt: Irradiance/Deger/Fixed): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processesing(df_final, df2_load, file):\n",
    "    \n",
    "    # Clean all the data\n",
    "    df_final, copy_final = prep(df_load.copy(), df2_load.copy())\n",
    "\n",
    "    beg_df, end_df, month_i = checking_gaps(df_final.copy(), file)\n",
    "\n",
    "    if type(beg_df) == type(df_final) and type(end_df) == type(df_final):\n",
    "        df = pd.concat([beg_df,df_final,end_df],axis=0,ignore_index=False)\n",
    "        copy_final =  pd.concat([beg_df,copy_final,end_df],axis=0,ignore_index=False)\n",
    "\n",
    "    #     index = np.hstack([list(beg_df.index.values),list(copy_final.index.values),list(end_df.index.values)])\n",
    "    #     df.index = index\n",
    "\n",
    "    elif type(beg_df) == type(df_final):\n",
    "        df = pd.concat([beg_df,df_final],axis=0,ignore_index=False)\n",
    "        copy_final =  pd.concat([beg_df,copy_final],axis=0,ignore_index=False)\n",
    "\n",
    "    #     index = np.hstack([list(beg_df.index.values),list(copy_final.index.values)])\n",
    "    #     df.index = index\n",
    "\n",
    "    elif type(end_df) == type(df_final):\n",
    "        df = pd.concat([df_final,end_df],axis=0,ignore_index=False)\n",
    "        copy_final =  pd.concat([copy_final,end_df],axis=0,ignore_index=False)\n",
    "\n",
    "    #     index = np.hstack([np.array(end_df.index.values),np.array(copy_final.index.values)])\n",
    "    #     df.index = index\n",
    "\n",
    "\n",
    "    nan_gap, test_gaps = df_mask(df.copy(), month_i, nan_gap = input('Include gaps smaller than (seconds): '))\n",
    "\n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR', title = 'Test Dataframe').show()\n",
    "        px.scatter(copy_final,x=copy_final.index,y='DirectIR', title = 'Observed Dataframe').show()\n",
    "\n",
    "    total_nan = df.isna().sum().sum()\n",
    "    total_values = df.size\n",
    "    mt_count = df.isna().all(axis=1).sum()\n",
    "    t_perc = round(total_nan/total_values * 100,3)\n",
    "    mt_perc = round(mt_count*5/total_values * 100,3)\n",
    "\n",
    "    print(\"Test Dataframe NaN Summary: \\n\")\n",
    "\n",
    "    print(f\"Percentage of NaN values due to System Outage: {mt_perc}% \\n\")\n",
    "\n",
    "    print(f\"Precentage of MAR NaN values: {round(t_perc-mt_perc,3)}% \\n\")\n",
    "\n",
    "    print(f\"Precentage of Total NaN values: {t_perc}% \\n\")\n",
    "\n",
    "    total_nan = copy_final.isna().sum().sum()\n",
    "    total_values = copy_final.size\n",
    "    mt_count = copy_final.isna().all(axis=1).sum()\n",
    "    t_perc = round(total_nan/total_values * 100,3)\n",
    "    mt_perc = round(mt_count*5/total_values * 100,3)\n",
    "\n",
    "    print(\"Observed Dataframe NaN Summary: \\n\")\n",
    "\n",
    "    print(f\"Percentage of NaN values due to System Outage: {mt_perc}% \\n\")\n",
    "\n",
    "    print(f\"Precentage of MAR NaN values: {round(t_perc-mt_perc,3)}% \\n\")\n",
    "\n",
    "    print(f\"Precentage of Total NaN values: {t_perc}%\")\n",
    "    \n",
    "    return df_final, copy_final, nan_gap, test_gaps\n",
    "    \n",
    "    \n",
    "df_final, copy_final, nan_gap, test_gaps = pre_processesing(df_load,df2_load,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_forward(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].fillna(method='ffill')\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "    \n",
    "imputation_dict['LOCF'] = fill_forward(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].interpolate(method='nearest')\n",
    "            \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['Nearest Neighbor'] = nearest(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_linear(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction = 'both')\n",
    "            \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['Linear Interpolation'] = interpolate_linear(df.copy()).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = kalman_StructTs(arr, model = \"StructTS\")\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['Kalman Smoothing'] = kalman(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = kalman_auto_arima(arr, model = \"auto.arima\")\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['ARIMA'] = ARIMA(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_decom(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = sea_decom(arr, algorithm = \"kalman\")\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['Seasonal Decomposition'] = seasonal_decom(df.copy()).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in df.index]\n",
    "    df['Day'] = [d.day for d in df.index]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum():\n",
    "            df_KNN = df[[col,'Day', 'Seconds']].copy()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_df = pd.DataFrame(scaler.fit_transform(df_KNN), columns = df_KNN.columns)\n",
    "            imputer = KNNImputer(n_neighbors=7,weights='distance')\n",
    "            knn_solar = pd.DataFrame(imputer.fit_transform(scaled_df),\n",
    "                                    columns=scaled_df.columns)\n",
    "            inverse_knn_solar = pd.DataFrame(scaler.inverse_transform(knn_solar),\n",
    "                                columns=knn_solar.columns, index=df_KNN.index)\n",
    "            df[col] = inverse_knn_solar[col]\n",
    "            \n",
    "    error_df = error_metric(df.drop(['Seconds','Day'], axis = 1), copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['K-Nearest Neighbor'] = knn(df.copy()).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoo_functions(df):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "    \n",
    "    df['seconds'] = [(df.index[-1] - time).total_seconds() for time in list(df.index)]\n",
    "    \n",
    "    df.index.name = 'Timestamp'\n",
    "    \n",
    "    df.to_csv('R_df_.csv')\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    subprocess.call(\"Rscript \" + cwd + \"/r_imputation.R\", shell=True)\n",
    "\n",
    "def zoo_spline(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df = pd.read_csv('spline_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = copy_final.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def forecast_interpolate(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df = pd.read_csv('forecast_int_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = copy_final.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def zoo_interpolation(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df = pd.read_csv('interpolation_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = copy_final.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def forecast_auto_arima(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df = pd.read_csv('forecast_arima.csv')\n",
    "    \n",
    "    df['Timestamp'] = copy_final.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "zoo_functions(df.copy())\n",
    "\n",
    "# imputation_dict['Forecast Auto.Arima'] = forecast_auto_arima(df_final.copy()).to_dict()\n",
    "imputation_dict['Zoo Interpolation'] = zoo_interpolation(df.copy()).to_dict()\n",
    "imputation_dict['Zoo Spline'] = zoo_spline(df.copy()).to_dict()\n",
    "imputation_dict['Forecast Interpolation'] = forecast_interpolate(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_loc(index,column,df):\n",
    "    i = df.index.get_loc(index)\n",
    "    \n",
    "    if column == 'DiffuseIR':\n",
    "        c = 0\n",
    "    elif column == 'DirectIR':\n",
    "        c = 1\n",
    "    elif column == 'WindSpeed':\n",
    "        c = 2\n",
    "    elif column == 'Temperature':\n",
    "        c = 3\n",
    "\n",
    "    return [i,c]    \n",
    "\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "def get_next_good_index(index,column,df):\n",
    "    while np.isnan(df.loc[index,column]):\n",
    "        index.replace(day=(index.day + 1))\n",
    "        index = nearest(df.index, index)\n",
    "        if index.day > df.index.day[-1]:\n",
    "            raise Exception('Out of Bounds Error')\n",
    "    \n",
    "    return index\n",
    "\n",
    "def get_previous_good_index(index,column,df):\n",
    "    while np.isnan(df.loc[index,column]):\n",
    "        index.replace(day=(index.day - 1))\n",
    "        index = nearest(df.index, index)\n",
    "        if index.day < 1:\n",
    "            raise Exception('Out of Bounds Error')\n",
    "        \n",
    "    return index\n",
    "    \n",
    "def small_hole_interpolation(index,column,df):\n",
    "    [i,c] = get_int_loc(index,column,df)\n",
    "    lasti = i-1\n",
    "    while np.isnan(df.iloc[lasti,c]):\n",
    "        lasti -= 1\n",
    "    nexti = i+1\n",
    "    while np.isnan(df.iloc[nexti,c]):\n",
    "        nexti += 1\n",
    "    last = df.iloc[lasti,c]\n",
    "    next = df.iloc[nexti,c]\n",
    "    dt1 = (df.index[nexti] - df.index[lasti])/np.timedelta64(1,'s')\n",
    "    m = (next-last)/dt1\n",
    "    dt2 = (df.index[i] - df.index[lasti])/np.timedelta64(1,'s')\n",
    "    new_value = last + m*dt2\n",
    "    return new_value\n",
    "\n",
    "\n",
    "def last_hole(index,column,df):\n",
    "    [i,c] = get_int_loc(index,column,df)\n",
    "    \n",
    "    l = 1\n",
    "    \n",
    "    while np.isnan(df.iloc[i+1,c]):\n",
    "        l = l + 1\n",
    "        \n",
    "        if i < len(df):\n",
    "            i = i + 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return [df.index[i],l]\n",
    "\n",
    "def interpolate(c,df):\n",
    "    for j in df.index:\n",
    "        if np.isnan(df.loc[j][c]):\n",
    "            [i,l] = last_hole(j,c,df)\n",
    "            dt = (i - j)/np.timedelta64(1,'s')\n",
    "            [start,_] = get_int_loc(j,c,df)\n",
    "            for x in range(start,start+l):\n",
    "                new_value = small_hole_interpolation(df.index[x],c,df)\n",
    "                df.at[df.index[x],c] = new_value\n",
    "            \n",
    "    return df[c]\n",
    "\n",
    "def other_fun(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    if df.iloc[[0,-1],:].isna().values.any():\n",
    "        \n",
    "        df.iloc[[0,-1],:] = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum(): continue\n",
    "            \n",
    "        df[col] = interpolate(col,df)\n",
    "                    \n",
    "    error_df = error_metric(df, copy_final, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['Prior Function'] = other_fun(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define function to impute missing values using MLP\n",
    "def impute_missing_values(df, prediction_window=24, num_hidden_layers=1, num_neurons=10):\n",
    "    \n",
    "    df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in df.index]\n",
    "    df['Day'] = [d.day for d in df.index]\n",
    "    df['Month'] = [d.month for d in df.index]\n",
    "    \n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    imputed_data = df.copy()\n",
    "\n",
    "    # Create a list of the columns that have missing values\n",
    "    missing_cols = imputed_data.columns[imputed_data.isnull().any()]\n",
    "\n",
    "    # Loop through each column with missing values\n",
    "    for col in missing_cols:\n",
    "        # Create a copy of the data for the current column\n",
    "        col_data = imputed_data[[col]].copy()\n",
    "\n",
    "        # Create a new column for each of the features of time\n",
    "        for i in range(prediction_window):\n",
    "            col_data['t-'+str(prediction_window-i)] = col_data[col].shift(prediction_window-i)\n",
    "\n",
    "        # Drop any rows with missing values\n",
    "        col_data.dropna(inplace=True)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train = col_data.drop(columns=[col])\n",
    "        y_train = col_data[[col]]\n",
    "        X_test = imputed_data.loc[imputed_data[col].isnull(), :].drop(columns=[col])\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Create the MLP and fit it to the training data\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(num_neurons,) * num_hidden_layers, activation='tanh')\n",
    "        mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict the missing values and add them to the imputed data\n",
    "        imputed_data.loc[imputed_data[col].isnull(), col] = mlp.predict(X_test_scaled)\n",
    "\n",
    "    return imputed_data\n",
    "\n",
    "# Call the impute_missing_values function and save the imputed data to a new CSV file\n",
    "imputation_dict['Prior Function'] = impute_missing_values(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MLP(df, copy_final = copy_final.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    col_names = [col for col in df.columns]\n",
    "    \n",
    "    df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in df.index]\n",
    "    df['Day'] = [d.day for d in df.index]\n",
    "    df['Month'] = [d.month for d in df.index]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if col in ['Seconds','Day','Month']: continue\n",
    "        \n",
    "        print(f'\\nFor target variable: {col}')\n",
    "\n",
    "        # Load the dataset and split into training and testing sets\n",
    "        X = df.drop(col_names, axis = 1).to_numpy() # Features matrix with shape (n_samples, n_features)\n",
    "        y = df[col].to_numpy() # Target vector with shape (n_samples,)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize the training and testing data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_std = scaler.fit_transform(X_train)\n",
    "        X_test_std = scaler.transform(X_test)\n",
    "\n",
    "        # Create a Lasso model for feature selection\n",
    "        lasso = Lasso(alpha=0.1)\n",
    "        lasso.fit(X_train_std, y_train)\n",
    "\n",
    "        # Print the coefficients of the Lasso model\n",
    "        print(\"Coefficients:\", lasso.coef_)\n",
    "\n",
    "        # Print the indices of the selected features\n",
    "        selected_indices = np.where(lasso.coef_ >= 0)[0]\n",
    "        if not selected_indices.any(): \n",
    "            (print(f'No features meet current criteria for: {col}'))\n",
    "            continue\n",
    "        print(\"Selected indices:\", df.drop(col_names, axis = 1).columns[selected_indices])\n",
    "\n",
    "        # Train an MLP regressor with the selected features\n",
    "        X_train_selected = X_train[:, selected_indices]\n",
    "        X_test_selected = X_test[:, selected_indices]\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(100,50), activation='tanh', solver='adam')\n",
    "        mlp.fit(X_train_selected, y_train)\n",
    "\n",
    "        # Evaluate the performance of the MLP regressor\n",
    "        score = mlp.score(X_test_selected, y_test)\n",
    "        print(\"Test score:\", score)\n",
    "\n",
    "MLP(df)\n",
    "# imputation_dict['MLP'] = MLP(df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def MLP_grid(df):\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "#     col_names = [col for col in df.columns]\n",
    "    \n",
    "    df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in df.index]\n",
    "    df['Day'] = [d.day for d in df.index]\n",
    "#     df['Month'] = [d.month for d in df.index]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if col in ['Seconds','Day','Month']: continue\n",
    "        \n",
    "        print(f'\\nFor target variable: {col}')\n",
    "\n",
    "        # Load the dataset and split into training and testing sets\n",
    "        X = df.drop([col], axis = 1).to_numpy() # Features matrix with shape (n_samples, n_features)\n",
    "        y = df[col].to_numpy() # Target vector with shape (n_samples,)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize the training and testing data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_std = scaler.fit_transform(X_train)\n",
    "        X_test_std = scaler.transform(X_test)\n",
    "\n",
    "        # Create a Lasso model for feature selection\n",
    "        lasso = Lasso(alpha=0.1)\n",
    "        lasso.fit(X_train_std, y_train)\n",
    "\n",
    "        # Print the coefficients of the Lasso model\n",
    "        print(\"Coefficients:\", lasso.coef_)\n",
    "\n",
    "        # Print the indices of the selected features\n",
    "        selected_indices = np.where(lasso.coef_ > 0)[0]\n",
    "        if not selected_indices.any(): \n",
    "            (print(f'No features meet current criteria for: {col}'))\n",
    "            continue\n",
    "        print(\"Selected indices:\", df.drop([col], axis = 1).columns[selected_indices])\n",
    "\n",
    "        # Train an MLP regressor with the selected features\n",
    "        X_train_selected = X_train[:, selected_indices]\n",
    "        X_test_selected = X_test[:, selected_indices]\n",
    "\n",
    "        # Define the hyperparameter grid to search over\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['adam', 'sgd'],\n",
    "        }\n",
    "\n",
    "        # Create an MLPRegressor with default hyperparameters\n",
    "        mlp = MLPRegressor()\n",
    "\n",
    "        # Use grid search to find the best hyperparameters\n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "        # Print the best hyperparameters and the test score of the best model\n",
    "        print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "        print(\"Test score:\", grid_search.score(X_test_selected, y_test))\n",
    "        \n",
    "MLP_grid(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(impute_dict):\n",
    "    imputation_df = pd.DataFrame.from_dict({(outerKey, innerKey): values for outerKey, innerDict in impute_dict.items() for innerKey, values in innerDict.items()}).T\n",
    "    r2_df = pd.DataFrame(imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).max(axis=0),columns = ['r2'])\n",
    "    r2_df['Method'] = imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).idxmax(axis=0).values\n",
    "    mae_df = pd.DataFrame(imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).min(axis=0),columns = ['mae'])\n",
    "    mae_df['Method'] = imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).idxmin(axis=0).values\n",
    "    rmse_df = pd.DataFrame(imputation_df.drop(['r2','mse','mae'], axis=0,level=1).min(axis=0),columns = ['rmse'])\n",
    "    rmse_df['Method'] = imputation_df.drop(['r2','mse','mae'], axis=0,level=1).idxmin(axis=0).values\n",
    "    for row in r2_df.index:\n",
    "        print(row)\n",
    "        print(f\"Optimal Imputation Method for {row}: {r2_df.loc[row]['Method'][0]}, R2 score: {round(r2_df.loc[row]['r2'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {mae_df.loc[row]['Method'][0]}, MAE score: {round(mae_df.loc[row]['mae'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {rmse_df.loc[row]['Method'][0]}, RMSE score: {round(rmse_df.loc[row]['rmse'],5)}\\n\")\n",
    "\n",
    "    return imputation_df\n",
    "\n",
    "print(f\"\\nImputation methods for {file} and error metrics in {year}, {month} modeled with NaN values in {year_2}, {month_2}.\\nIncluding gaps smaller than {nan_gap} seconds.\\n\")\n",
    "performance(imputation_dict.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
