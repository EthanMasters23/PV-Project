{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Kalman Smoothing using R objects\n",
    "import rpy2.robjects as robjects\n",
    "# import R packages\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Impute TS\n",
    "imputeTS = importr('imputeTS') \n",
    "kalman_StructTs = robjects.r['na_kalman']\n",
    "sea_decom = robjects.r['na.seadec']\n",
    "sea_split = robjects.r['na.seasplit']\n",
    "\n",
    "import sys\n",
    "module_path = re.sub(r'Notebooks','Python Scripts',os.getcwd())\n",
    "sys.path.append(module_path)\n",
    "from pv_modules import *\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaner(path_list,file):\n",
    "    \n",
    "    df = pd.read_csv(path_list[0],sep=\"\\t|,\",engine='python')\n",
    "        \n",
    "    # === In case a file isn't stored properly or empty === #\n",
    "    if df.empty:\n",
    "        raise Exception(\"Loaded an empty dataframe\")\n",
    "\n",
    "    # ==== reshaping df for timestap & adjusted headers ==== #\n",
    "    df = reshape_df(df,file)\n",
    "\n",
    "    # === filling gaps in time intervals === #\n",
    "    df,_ = add_missing_times(df)\n",
    "\n",
    "    # # ==== Using PvLib to remove nightime values === #\n",
    "    df = remove_night(df)\n",
    "\n",
    "    if file == 'Irradiance':\n",
    "        # === Removing Values for Irradiance === #\n",
    "        df = clean_irradiance_values(df)\n",
    "\n",
    "    else:\n",
    "        # === Removing Values for Deger & Fixed === #\n",
    "        df = clean_deger_fixed_values(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrounding_month(month, year , cond):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the surrounding month and year based on the input month, year, and condition.\n",
    "\n",
    "    Args:\n",
    "        month (int): The input month (1-12).\n",
    "        year (int): The input year.\n",
    "        cond (str): The condition indicating the desired surrounding month. Possible values are 'prev' or 'next'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the surrounding month and year.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    month_dict = {0:'dec',13:'jan',8:'aug',9:'sep',10:'oct',11:'nov',12:'dec',1:'jan',2:'feb',3:'mar',4:'apr',5:'may',6:'jun',7:'jul'}\n",
    "    if cond == 'prev':\n",
    "        if month == 1:\n",
    "            year -= 1\n",
    "        return month_dict[month-1], year\n",
    "    else:\n",
    "        if month == 12:\n",
    "            year += 1\n",
    "        return month_dict[month+1], year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_simulation(df):\n",
    "    \n",
    "    gap_length = int(int(input(\"Gap size (seconds): \"))/ 12) # Avg observation interval\n",
    "    \n",
    "    df = df.dropna()\n",
    "        \n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    index_list = df.reset_index().index\n",
    "    \n",
    "    while True:\n",
    "        selected_index = random.choice([index for index in index_list if index < index_list[-1] - gap_length])\n",
    "        if df.index[selected_index].day == df.index[selected_index+gap_length].day:\n",
    "            break\n",
    "            \n",
    "    df.iloc[selected_index:selected_index+gap_length, :] = np.nan\n",
    "\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support Files/',os.getcwd())\n",
    "    df.to_csv(datapath + 'test_data.csv')\n",
    "\n",
    "    return df, copy_df, gap_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nan_gaps_indexes(pre_df, date):\n",
    "    \n",
    "    test_gaps = {}\n",
    "    for col in range(len(pre_df.columns)):\n",
    "        inx_ind = []\n",
    "        test_gaps[pre_df.columns[col]] = []\n",
    "        for index in range(len(pre_df.index)):\n",
    "            if pre_df.index[index].month != int(date): continue\n",
    "            if index in inx_ind: continue\n",
    "            c = 0\n",
    "            while np.isnan(pre_df.iloc[index+c,col]) and pre_df.index[index+c] != pre_df.index[-1]:\n",
    "                inx_ind += [index+c]\n",
    "                c += 1\n",
    "            if not c and not np.isnan(pre_df.iloc[index+c,col]): continue\n",
    "            test_gaps[pre_df.columns[col]] += list(range(index,index+c))\n",
    "            \n",
    "    return test_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_missing_data_beg_end_month(df, file):\n",
    "    \n",
    "    month_i = df.index[0].month\n",
    "\n",
    "    beg_ind = False\n",
    "    end_ind = False\n",
    "    path_list = get_file_paths(file)\n",
    "\n",
    "    beg_threshold = int(len(df) / df.iloc[-1].name.day * 5)\n",
    "    end_threshold = int(len(df) / df.iloc[-1].name.day * (df.iloc[-1].name.day - 4))\n",
    "\n",
    "    # Check for missing data in the beginning of the month\n",
    "    if df.iloc[0:beg_threshold, :].isna().sum().sum() / df.iloc[0:beg_threshold, :].size * 100 > 5:\n",
    "        if month_i == 1 and df.index[0].year == 2021:\n",
    "            print(\"January 2021 is the first month observed, can't join prior month.\")\n",
    "            return False, False, month_i\n",
    "        month, year = surrounding_month(df.iloc[0].name.month, df.iloc[0].name.year, 'prev')\n",
    "        path_list = path_function_extended(year, month, '', '', path_list)\n",
    "        load_beg = df_cleaner(path_list, file)\n",
    "        load_beg = load_beg.drop(columns='GlobalIR', errors='ignore')\n",
    "        beg_ind = True\n",
    "\n",
    "    # Check for missing data at the end of the month\n",
    "    if df.iloc[end_threshold:-1, :].isna().sum().sum() / df.iloc[end_threshold:-1, :].size * 100 > 5:\n",
    "        if month_i == 2 and df.index[0].year == 2023:\n",
    "            print(\"February 2021 is the last month observed, can't join following month.\")\n",
    "            return False, False, month_i\n",
    "        month, year = surrounding_month(df.iloc[0].name.month, df.iloc[0].name.year, '')\n",
    "        path_list = path_function_extended(year, month, '', '', path_list)\n",
    "        load_end = df_cleaner(path_list, file)\n",
    "        load_end = load_end.drop(columns='GlobalIR', errors='ignore')\n",
    "        end_ind = True\n",
    "\n",
    "    if beg_ind and end_ind:\n",
    "        return load_beg, load_end, month_i\n",
    "    elif beg_ind:\n",
    "        return load_beg, False, month_i\n",
    "    elif end_ind:\n",
    "        return False, load_end, month_i\n",
    "    else:\n",
    "        return False, False, month_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imputation_errors(imputed_df, copy_df, test_gaps):\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for col in imputed_df.columns:\n",
    "        \n",
    "        pred_val = imputed_df[col].iloc[test_gaps[col]]\n",
    "        test_val = copy_df[col].iloc[test_gaps[col]]\n",
    "\n",
    "        mae = mean_absolute_error(test_val,pred_val)\n",
    "        mse = mean_squared_error(test_val, pred_val)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(test_val, pred_val)\n",
    "        \n",
    "        error_dict[col] = [mae,mse,rmse,r2]\n",
    "\n",
    "        print(f\"For {col}\")\n",
    "        print(f\"Mean Absolute Error: {mae}\")\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse}\")\n",
    "        print(f\"R2 Score: {r2} \\n\")\n",
    "        \n",
    "    error_df = pd.DataFrame(error_dict).T\n",
    "    error_df.columns = ['mae','mse','rmse','r2']\n",
    "    \n",
    "    print(error_df)\n",
    "    \n",
    "    px.bar(error_df, x = error_df.index, y = error_df['r2']).show()\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(year, month, year_2, month_2, file):\n",
    "    if not re.search(r'\\d{4}',year):\n",
    "        raise Exception(f\"Incorret Input: {year}\")\n",
    "    elif not re.search(r'[A-Za-z]{3}',month):\n",
    "        raise Exception(f\"Incorret Input: {month}\")\n",
    "    elif not re.search(r'[A-Za-z]{3}',month_2):\n",
    "        raise Exception(f\"Incorret Input: {month_2}\")\n",
    "    elif not re.search(r'\\d{4}',year_2):\n",
    "        raise Exception(f\"Incorret Input: {year_2}\")\n",
    "    elif not [file_i for file_i in ['Irradiance','Deger','Fixed'] if re.search(fr'{file}',file_i)]:\n",
    "        raise Exception(f\"Incorret Input: File\")\n",
    "    else:\n",
    "        path_list = get_file_paths(file)\n",
    "        path_list = path_function_extended(year,month,year_2,month_2,path_list)\n",
    "        df = df_cleaner([path_list[0]],file)\n",
    "        df_2 = df_cleaner([path_list[1]],file)\n",
    "    return df, df_2,file, year_2, month_2, year, month\n",
    "\n",
    "# = Load all the Data = #\n",
    "test_df, source_df, file, year_2, month_2, year, month = main(year = input(\"Year (format: YYYY): \"),month = input(\"Month (format: jul): \"),\n",
    "     year_2 = input(\"Second Year (format: YYYY): \"),month_2 = input(\"Second Month (format: jul): \"),\n",
    "     file = input(\"File (opt: Irradiance/Deger/Fixed): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessesing(test_df, source_df, file):\n",
    "    \n",
    "    test_df, base_df, syn_gap = missing_value_simulation(test_df.copy())\n",
    "        \n",
    "    beg_df, end_df, test_month = identify_missing_data_beg_end_month(test_df.copy(), file)\n",
    "\n",
    "    if type(beg_df) == type(test_df) and type(end_df) == type(test_df):\n",
    "        test_df = pd.concat([beg_df,test_df,end_df],axis=0,ignore_index=False)\n",
    "        base_df =  pd.concat([beg_df,base_df,end_df],axis=0,ignore_index=False)\n",
    "\n",
    "    elif type(beg_df) == type(test_df):\n",
    "        test_df = pd.concat([beg_df,test_df],axis=0,ignore_index=False)\n",
    "        base_df =  pd.concat([beg_df,base_df],axis=0,ignore_index=False)\n",
    "\n",
    "    elif type(end_df) == type(test_df):\n",
    "        test_df = pd.concat([test_df,end_df],axis=0,ignore_index=False)\n",
    "        base_df =  pd.concat([base_df,end_df],axis=0,ignore_index=False)\n",
    "\n",
    "    test_gaps = map_nan_gaps_indexes(test_df.copy(), test_month)\n",
    "    \n",
    "    return test_df, base_df, test_gaps syn_gap\n",
    "    \n",
    "test_df, base_df, test_gaps, syn_gap = preprocessesing(test_df,source_df,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_forward(df, df_copy = base_df.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "\n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].fillna(method='ffill')\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "    \n",
    "imputation_dict['LOCF'] = fill_forward(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].interpolate(method='nearest')\n",
    "            \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "imputation_dict['Nearest Neighbor'] = nearest(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = kalman_StructTs(arr, model = \"StructTS\")\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "imputation_dict['Kalman Smoothing'] = kalman(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = kalman_StructTs(arr, model = \"auto.arima\")\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "imputation_dict['ARIMA'] = ARIMA(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_decom(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        arr = np.ndarray.tolist(df[col].values)\n",
    "        arr = robjects.FloatVector(arr)\n",
    "\n",
    "        df[col] = sea_decom(arr, algorithm = \"kalman\")\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "imputation_dict['Seasonal Decomposition'] = seasonal_decom(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoo_functions(df):\n",
    "    \n",
    "    df.iloc[[-1,0],:] = 0\n",
    "    \n",
    "    df['seconds'] = [(df.index[-1] - time).total_seconds() for time in list(df.index)]\n",
    "    \n",
    "    df.index.name = 'Timestamp'\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R',current_dir)\n",
    "\n",
    "    df.to_csv(datapath + '/R_df_.csv')\n",
    "    \n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R',os.getcwd())\n",
    "    os.chdir(datapath)\n",
    "    subprocess.call(\"Rscript \" + datapath + \"/r_imputation.R\", shell=True)\n",
    "    os.chdir(current_dir)\n",
    "def zoo_spline(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    \n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R/',os.getcwd())\n",
    "    df = pd.read_csv(datapath + 'spline_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = df_copy.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def forecast_interpolate(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R/',os.getcwd())\n",
    "    df = pd.read_csv(datapath + 'forecast_int_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = df_copy.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def zoo_interpolation(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R/',os.getcwd())\n",
    "    df = pd.read_csv(datapath + 'interpolation_df.csv')\n",
    "    \n",
    "    df['Timestamp'] = df_copy.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "def forecast_auto_arima(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "    datapath = re.sub(r'Notebooks|Python Scripts','Support_Scripts/R/',os.getcwd())\n",
    "    df = pd.read_csv(datapath + 'forecast_arima.csv')\n",
    "    \n",
    "    df['Timestamp'] = df_copy.index\n",
    "    \n",
    "    df = df.set_index('Timestamp')\n",
    "    \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    if 'DiffuseIR' in df.columns:\n",
    "        px.scatter(df,x=df.index,y='DirectIR').show()\n",
    "        \n",
    "    return error_df\n",
    "\n",
    "zoo_functions(test_df.copy())\n",
    "\n",
    "# imputation_dict['Forecast Auto.Arima'] = forecast_auto_arima(df.copy()).to_dict()\n",
    "imputation_dict['Zoo Interpolation'] = zoo_interpolation(test_df.copy()).to_dict()\n",
    "imputation_dict['Zoo Spline'] = zoo_spline(test_df.copy()).to_dict()\n",
    "imputation_dict['Forecast Interpolation'] = forecast_interpolate(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "        \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if not df[col].isna().sum().sum(): continue\n",
    "\n",
    "        df[col] = df[col].interpolate(method='time', limit_direction='both')\n",
    "            \n",
    "    error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "imputation_dict['Python Interpolate'] = interpolate(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "kalman_StructTs = robjects.r['na.kalman']\n",
    "\n",
    "def sm_interpolate(df, df_copy = base_df, test_gaps=test_gaps):\n",
    "\n",
    "    # Convert the DataFrame to an R data frame\n",
    "    pandas2ri.activate()\n",
    "    r_data_frame = pandas2ri.py2rpy(df)\n",
    "\n",
    "    # Load the imputeTS package\n",
    "    robjects.r('library(imputeTS)')\n",
    "\n",
    "    # Use the na.interp function for imputation\n",
    "    imputed_data_frame = robjects.r('na.interp')(r_data_frame, option='arima')\n",
    "\n",
    "    # Convert the imputed data frame back to a pandas DataFrame\n",
    "    imputed_data_frame = pandas2ri.ri2py(imputed_data_frame)\n",
    "    \n",
    "    error_df = calculate_imputation_errors(imputed_data_frame, df_copy, test_gaps)\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "# imputation_dict['SM Interpolate'] = sm_interpolate(test_df.copy()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(impute_dict):\n",
    "    imputation_df = pd.DataFrame.from_dict({(outerKey, innerKey): values for outerKey, innerDict in impute_dict.items() for innerKey, values in innerDict.items()}).T\n",
    "    r2_df = pd.DataFrame(imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).max(axis=0),columns = ['r2'])\n",
    "    r2_df['Method'] = imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).idxmax(axis=0).values\n",
    "    mae_df = pd.DataFrame(imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).min(axis=0),columns = ['mae'])\n",
    "    mae_df['Method'] = imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).idxmin(axis=0).values\n",
    "    rmse_df = pd.DataFrame(imputation_df.drop(['r2','mse','mae'], axis=0,level=1).min(axis=0),columns = ['rmse'])\n",
    "    rmse_df['Method'] = imputation_df.drop(['r2','mse','mae'], axis=0,level=1).idxmin(axis=0).values\n",
    "    for row in r2_df.index:\n",
    "        print(row)\n",
    "        print(f\"Optimal Imputation Method for {row}: {r2_df.loc[row]['Method'][0]}, R2 score: {round(r2_df.loc[row]['r2'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {mae_df.loc[row]['Method'][0]}, MAE score: {round(mae_df.loc[row]['mae'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {rmse_df.loc[row]['Method'][0]}, RMSE score: {round(rmse_df.loc[row]['rmse'],5)}\\n\")\n",
    "\n",
    "    return imputation_df\n",
    "\n",
    "if test == 'Synthetic':\n",
    "    print(f\"\\nImputation methods for {file} and error metrics in {year}, {month} with a {syn_gap} second gap from {test_df.index[test_gaps[test_df.columns[0]][0]]} - {test_df.index[test_gaps[test_df.columns[0]][-1]]}.\\n\")\n",
    "\n",
    "    \n",
    "print(f\"\\nImputation methods for {file} and error metrics in {year}, {month} modeled with NaN values in {year_2}, {month_2}.\\nIncluding gaps larger than {nan_gap_i} and smaller than {nan_gap_f} seconds.\\n\")\n",
    "performance(imputation_dict.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
