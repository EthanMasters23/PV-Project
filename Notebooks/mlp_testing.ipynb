{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "module_path = re.sub(r'Notebooks','Python Scripts',os.getcwd())\n",
    "sys.path.append(module_path)\n",
    "from performance_helper import *\n",
    "from ml_helper import *\n",
    "\n",
    "# from warnings import simplefilter\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(year, month, file, gap_length):\n",
    "    if not re.search(r'\\d{4}',year):\n",
    "        raise Exception(f\"Incorret Input: {year}\")\n",
    "    elif not re.search(r'[A-Za-z]{3}',month):\n",
    "        raise Exception(f\"Incorret Input: {month}\")\n",
    "    elif not [file_i for file_i in ['Irradiance','Deger','Fixed'] if re.search(fr'{file}',file_i)]:\n",
    "        raise Exception(f\"Incorret Input: File\")\n",
    "    else:\n",
    "        path_list = get_file_paths(file)\n",
    "        path_list = path_function_extended(year,month,None,None,path_list)\n",
    "        df = df_cleaner([path_list[0]],file)\n",
    "    return df, file, year, month, gap_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_validation_df(test_df, file, gap_length):\n",
    "        \n",
    "    test_df, base_df = missing_value_simulation(test_df, gap_length)\n",
    "\n",
    "    test_df, base_df, test_month = missing_data_beg_end_month(test_df, base_df, file)\n",
    "\n",
    "    test_gaps = map_nan_gaps_indexes(test_df, test_month)\n",
    "            \n",
    "    return test_df, base_df, test_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# = load all the Data = #\n",
    "test_df, file, year, month, gap_length = main(year = input(\"Year (format: YYYY): \"),month = input(\"Month (format: jul): \"),\n",
    "     file = input(\"File (opt: Irradiance/Deger/Fixed): \"), gap_length = int(input(\"Gap size (seconds): \")))\n",
    "\n",
    "# = load test data = #\n",
    "test_df, base_df, test_gaps = process_validation_df(test_df, file, gap_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapath = re.sub(\"Notebooks\",\"Support Files/\",os.getcwd())\n",
    "ml_df = pd.read_csv(datapath + 'ml_features.csv',index_col=0)\n",
    "ml_df = reshape_ml(test_df, ml_df)\n",
    "\n",
    "# = if reshaping = #\n",
    "ml_df = resample_ml(test_df, ml_df, freq = '20s')\n",
    "ml_df = interpolate(ml_df)\n",
    "\n",
    "imputation_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# px.scatter(test_df, y='DirectIR').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# px.scatter(ml_df, y='Direct Shortwave Radiation (W/mÂ²) (sfc)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import neptune\n",
    "import neptune.integrations.sklearn as npt_utils\n",
    "\n",
    "def MLP_single(df, ml_df, df_copy = base_df.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    ml_df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in ml_df.index]\n",
    "    ml_df['Day'] = [d.day for d in ml_df.index]\n",
    "    \n",
    "    col_indx = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        if col != 'Temperature':continue\n",
    "                \n",
    "#         print(f'\\n\\nFor target variable: {col}\\n')\n",
    "        \n",
    "        test_ml_df = ml_df.copy()\n",
    "        \n",
    "        test_ml_df[col] = df[df.index.isin(ml_df.index)][col]\n",
    "        \n",
    "        X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "        y = test_ml_df[col].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "#         y_train = scaler.fit_transform(y_train)\n",
    "#         y_test = scaler.fit_transform(y_test)\n",
    "        \n",
    "        lasso = Lasso(alpha=0.1) # range of lasso (0.001 to 10)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "        selected_indices = np.where(lasso.coef_ != 0)[0]\n",
    "        \n",
    "        if not selected_indices.any(): \n",
    "            (print(f'No features meet current criteria for: {col}'))\n",
    "            continue\n",
    "            \n",
    "#         print(\"Coefficients:\", list(zip(lasso.coef_,test_ml_df.drop([col], axis = 1).columns)))\n",
    "\n",
    "        X_train_selected = X_train[:, selected_indices]\n",
    "        X_test_selected = X_test[:, selected_indices]\n",
    "        \n",
    "        parameters = {\n",
    "            \"hidden_layer_sizes\": (60,85),\n",
    "            \"activation\": \"tanh\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"learning_rate_init\": 0.001,\n",
    "            \"max_iter\": 300,\n",
    "            \"alpha\": 0.0001,\n",
    "            \"beta_1\": 0.9,\n",
    "            \"beta_2\": 0.999,\n",
    "            \"epsilon\": 1e-8\n",
    "        }\n",
    "        \n",
    "        # == Model == #\n",
    "            \n",
    "        mlp = MLPRegressor(**parameters)\n",
    "        \n",
    "        mlp.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # == Model == #\n",
    "        \n",
    "        run = neptune.init_run(\n",
    "            project=\"ethanmasters/PV-Solar-MLP\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMWZhYmFiYi0zYWEzLTQ3NTMtYmMyOS1jZjAzYjY0N2EwYjgifQ==\",\n",
    "            name=\"MLP-DiffuseIR\",\n",
    "            tags=[\"MLPRegressor\", \"regression\", \"Temperature\"],\n",
    "        )\n",
    "        \n",
    "        run[\"parameters\"] = parameters\n",
    "        \n",
    "        run[\"mlp_summary\"] = npt_utils.create_regressor_summary(mlp, X_train_selected, X_test_selected, y_train, y_test)\n",
    "\n",
    "        run.stop()\n",
    "        \n",
    "        # == Model == #\n",
    "                \n",
    "#         print(\"Selected features: \", list(test_ml_df.drop([col], axis = 1).columns))\n",
    "#         print(\"Target: \", col)\n",
    "#         print(\"Number of layers: \", mlp.n_layers_)\n",
    "#         print(\"Number of outputs: \", mlp.n_outputs_)\n",
    "#         print(\"Output activation: \", mlp.out_activation_)\n",
    "#         print(\"Number of iterations:\", mlp.n_iter_)\n",
    "#         print(\"Best loss: \", mlp.best_loss_)\n",
    "#         print(\"Current loss: \", mlp.loss_)\n",
    "#         print(\"Number of training samples seen: \", mlp.t_)\n",
    "        \n",
    "#         print(\"\\nLoss Curve: \")\n",
    "        \n",
    "#         px.line(x = range(mlp.n_iter_), y = mlp.loss_curve_, title = \"Loss Curve\").show()\n",
    "        \n",
    "#         score = mlp.score(X_test_selected, y_test)\n",
    "#         print(\"\\nTest score:\", score)\n",
    "        \n",
    "        if test_gaps:\n",
    "            imputation_values = scaler.fit_transform(test_ml_df.drop([col], axis = 1).iloc[test_gaps[col],:].to_numpy()[:, selected_indices])\n",
    "            prediction = mlp.predict(imputation_values)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            df.iloc[test_gaps[col],:][col] = prediction\n",
    "            \n",
    "        col_indx += 1\n",
    "        \n",
    "#         print(\"\\nPerumation Importance: \")\n",
    "\n",
    "#         feature_selection(test_ml_df.drop([col], axis = 1).iloc[:, selected_indices], mlp, X_train_selected, y_train)\n",
    "        \n",
    "#         multilinear_feature_selection(test_ml_df.drop([col], axis = 1).iloc[:, selected_indices], X_test_selected)\n",
    "        \n",
    "    if test_gaps:\n",
    "        error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "        return error_df.to_dict()\n",
    "MLP_single(test_df.copy(),ml_df.copy())\n",
    "\n",
    "# imputation_dict['MLP'] = MLP_single(test_df.copy(),ml_df.copy())\n",
    "# print(imputation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MLP_multi(df, ml_df, df_copy = base_df.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    ml_df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in ml_df.index]\n",
    "    ml_df['Day'] = [d.day for d in ml_df.index]\n",
    "    \n",
    "    col_indx = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "                \n",
    "        print(f'\\n\\nFor target variable: {col}\\n')\n",
    "        \n",
    "        test_ml_df = ml_df.copy()\n",
    "        \n",
    "        test_ml_df[col] = df[df.index.isin(ml_df.index)][col]\n",
    "        \n",
    "        if col == 'GlobalIR':\n",
    "            test_ml_df = test_ml_df.drop(['Wind Speed (km/h) (10 m)','Wind Direction (Â°) (10 m)',\n",
    "                                           'Cloud Cover Total (%) (sfc)', 'Day'],axis=1)\n",
    "        if col == 'DirectIR':\n",
    "            test_ml_df = test_ml_df.drop(['Wind Speed (km/h) (10 m)','Wind Direction (Â°) (10 m)',\n",
    "                                           'Cloud Cover Total (%) (sfc)', 'Day'],axis=1)\n",
    "        if col == 'DiffuseIR':\n",
    "            test_ml_df = test_ml_df.drop(['Wind Speed (km/h) (10 m)','Wind Direction (Â°) (10 m)',\n",
    "                                           'Cloud Cover Total (%) (sfc)', 'Day', 'Temperature (Â°C) (2 m elevation corrected)'],axis=1)\n",
    "        if col == 'Temperature':\n",
    "            test_ml_df = test_ml_df.drop(['Wind Speed (km/h) (10 m)','Wind Direction (Â°) (10 m)',\n",
    "                                           'Cloud Cover Total (%) (sfc)', 'Day'],axis=1)\n",
    "        if col == 'WindSpeed':\n",
    "            test_ml_df = test_ml_df.drop(['Wind Direction (Â°) (10 m)','Cloud Cover Total (%) (sfc)',\n",
    "                                          'Day', 'Temperature (Â°C) (2 m elevation corrected)', 'Seconds'],axis=1)\n",
    "                       \n",
    "        X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "        y = test_ml_df[col].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_std = scaler.fit_transform(X_train)\n",
    "        X_test_std = scaler.fit_transform(X_test)\n",
    "        \n",
    "        \n",
    "        if col == 'GlobalIR':\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(25,25,25), activation='tanh', solver='adam')\n",
    "            mlp.fit(X_train_std, y_train)\n",
    "            score = mlp.score(X_test_std, y_test)\n",
    "        if col == 'DirectIR':\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(70,95), activation='tanh', solver='adam')\n",
    "            mlp.fit(X_train_std, y_train)\n",
    "            score = mlp.score(X_test_std, y_test)\n",
    "        if col == 'DiffuseIR':\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(60,85), activation='tanh', solver='adam')\n",
    "            mlp.fit(X_train_std, y_train)\n",
    "            score = mlp.score(X_test_std, y_test)\n",
    "        if col == 'Temperature':\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(60,85), activation='tanh', solver='adam')\n",
    "            mlp.fit(X_train_std, y_train)\n",
    "            score = mlp.score(X_test_std, y_test)\n",
    "        if col == 'WindSpeed':\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(70,95), activation='relu', solver='adam')\n",
    "            mlp.fit(X_train_std, y_train)\n",
    "            score = mlp.score(X_test_std, y_test)\n",
    "                \n",
    "        print(\"Selected features: \", list(test_ml_df.drop([col], axis = 1).columns))\n",
    "        print(\"Target: \", col)\n",
    "        print(\"Number of layers: \", mlp.n_layers_)\n",
    "        print(\"Number of outputs: \", mlp.n_outputs_)\n",
    "        print(\"Output activation: \", mlp.out_activation_)\n",
    "        print(\"Number of iterations:\", mlp.n_iter_)\n",
    "        print(\"Best loss: \", mlp.best_loss_)\n",
    "        print(\"Current loss: \", mlp.loss_)\n",
    "        print(\"Number of training samples seen: \", mlp.t_)\n",
    "        \n",
    "        print(\"\\nLoss Curve: \")\n",
    "        \n",
    "        px.line(x = range(mlp.n_iter_), y = mlp.loss_curve_, title = \"Loss Curve\").show()\n",
    "        \n",
    "        print(\"\\nTest score:\", score)\n",
    "        \n",
    "        if test_gaps:\n",
    "            imputation_values = scaler.fit_transform(test_ml_df.drop([col], axis = 1).iloc[test_gaps[col],:].to_numpy()[:, selected_indices])\n",
    "            prediction = mlp.predict(imputation_values)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            df.iloc[test_gaps[col],:][col] = prediction\n",
    "            \n",
    "        col_indx += 1\n",
    "        \n",
    "        print(\"\\nPerumation Importance: \")\n",
    "        \n",
    "        feature_selection(test_ml_df.drop([col], axis = 1), mlp, X_train_std, y_train)\n",
    "        \n",
    "        multilinear_feature_selection(test_ml_df.drop([col], axis = 1), X_test_std)\n",
    "\n",
    "    if test_gaps:\n",
    "        error_df = calculate_imputation_errors(df, df_copy, test_gaps)\n",
    "        return error_df.to_dict()\n",
    "   \n",
    "imputation_dict['MLP'] = MLP_multi(test_df.copy(),ml_df.copy())\n",
    "# print(imputation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(help(MLPRegressor))\n",
    "print(dir(MLPRegressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MLP_2(df, ml_df, df_copy = base_df.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    ml_df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in ml_df.index]\n",
    "    ml_df['Day'] = [d.day for d in ml_df.index]\n",
    "    \n",
    "    col_indx = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "                \n",
    "        if col != 'DiffuseIR':continue\n",
    "            \n",
    "        print(f'\\n\\nFor target variable: {col}\\n')\n",
    "        \n",
    "        test_ml_df = ml_df.copy()\n",
    "        \n",
    "        test_ml_df[col] = df[df.index.isin(ml_df.index)][col]\n",
    "            \n",
    "        X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "        y = test_ml_df[col].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_std = scaler.fit_transform(X_train)\n",
    "        X_test_std = scaler.fit_transform(X_test)\n",
    "        \n",
    "        lasso = Lasso(alpha=0.1) # range of lasso (0.001 to 10)\n",
    "        lasso.fit(X_train_std, y_train)\n",
    "        \n",
    "#         print(\"Coefficients:\", list(zip(lasso.coef_,test_ml_df.drop([col], axis = 1).columns)))\n",
    "\n",
    "        selected_indices = np.where(lasso.coef_ != 0)[0]\n",
    "        \n",
    "        if not selected_indices.any(): \n",
    "            (print(f'No features meet current criteria for: {col}'))\n",
    "            continue\n",
    "            \n",
    "#         print(\"\\nSelected indices:\", list(test_ml_df.drop([col], axis = 1).columns[selected_indices]))\n",
    "\n",
    "        X_train_selected = X_train_std[:, selected_indices]\n",
    "        X_test_selected = X_test_std[:, selected_indices]\n",
    "\n",
    "\n",
    "        param_grid = {\n",
    "                'hidden_layer_sizes': [(5,),(10,),(15,),(20,),(5,5),(10,10),(15,15),(20,20)],\n",
    "            'activation': ['relu'],\n",
    "            'solver': ['adam']\n",
    "            } \n",
    "\n",
    "\n",
    "        mlp = MLPRegressor()\n",
    "        \n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "        print(f\"Best hyperparameters for {col}: \", grid_search.best_params_)\n",
    "        print(\"Test score: \", grid_search.score(X_test_selected, y_test))\n",
    "        \n",
    "        print(\"Mean test score: \", grid_search.cv_results_['mean_test_score'])\n",
    "        print(\"Mean fit time: \", grid_search.cv_results_['mean_fit_time'])\n",
    "        print(\"Mean score time: \", grid_search.cv_results_['mean_score_time'])\n",
    "                \n",
    "\n",
    "MLP_2(test_df.copy(),ml_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_2(df, ml_df, df_copy = base_df.copy(), test_gaps=test_gaps):\n",
    "    \n",
    "    ml_df['Seconds'] = [(time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds() for time in ml_df.index]\n",
    "    ml_df['Day'] = [d.day for d in ml_df.index]\n",
    "    \n",
    "    col_indx = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "                \n",
    "        if col != 'DiffuseIR':continue\n",
    "            \n",
    "        print(f'\\n\\nFor target variable: {col}\\n')\n",
    "        \n",
    "        test_ml_df = ml_df.copy()\n",
    "        \n",
    "        test_ml_df[col] = df[df.index.isin(ml_df.index)][col]\n",
    "            \n",
    "        X = test_ml_df.drop([col], axis = 1).to_numpy()\n",
    "        y = test_ml_df[col].to_numpy()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_std = scaler.fit_transform(X_train)\n",
    "        X_test_std = scaler.fit_transform(X_test)\n",
    "        \n",
    "        lasso = Lasso(alpha=0.1) # range of lasso (0.001 to 10)\n",
    "        lasso.fit(X_train_std, y_train)\n",
    "        \n",
    "#         print(\"Coefficients:\", list(zip(lasso.coef_,test_ml_df.drop([col], axis = 1).columns)))\n",
    "\n",
    "        selected_indices = np.where(lasso.coef_ != 0)[0]\n",
    "        \n",
    "        if not selected_indices.any(): \n",
    "            (print(f'No features meet current criteria for: {col}'))\n",
    "            continue\n",
    "            \n",
    "#         print(\"\\nSelected indices:\", list(test_ml_df.drop([col], axis = 1).columns[selected_indices]))\n",
    "\n",
    "        X_train_selected = X_train_std[:, selected_indices]\n",
    "        X_test_selected = X_test_std[:, selected_indices]\n",
    "        \n",
    "#         if col == 'WindSpeed':\n",
    "#             param_grid = {\n",
    "#                 'hidden_layer_sizes': [(60,85),(70,95),(65,90)],\n",
    "#                 'activation': ['relu'],\n",
    "#                 'solver': ['adam']\n",
    "#             }\n",
    "#         elif col == 'GlobalIR':\n",
    "#             param_grid = {\n",
    "#                 'hidden_layer_sizes': [(25,25,25),(60,85),(70,95),(65,90)],\n",
    "#                 'activation': ['tanh'],\n",
    "#                 'solver': ['adam']\n",
    "#             }\n",
    "#         else:\n",
    "#             param_grid = {\n",
    "#                 'hidden_layer_sizes': [(60,85),(70,95),(65,90)],\n",
    "#                 'activation': ['tanh'],\n",
    "#                 'solver': ['adam']\n",
    "#             }\n",
    "\n",
    "        if col == 'WindSpeed':\n",
    "            param_grid = {\n",
    "                'hidden_layer_sizes': [(5,),(10,),(15,),(20,),(5,5),(10,10),(15,15),(20,20)],\n",
    "            'activation': ['relu'],\n",
    "            'solver': ['adam']\n",
    "            } \n",
    "        else:\n",
    "            param_grid = {\n",
    "                'hidden_layer_sizes': [(5,),(10,),(15,),(20,),(5,5),(10,10),(15,15),(20,20)],\n",
    "                'activation': ['tanh'],\n",
    "                'solver': ['adam']\n",
    "            }\n",
    "\n",
    "        mlp = MLPRegressor()\n",
    "        \n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "        print(f\"Best hyperparameters for {col}: \", grid_search.best_params_)\n",
    "        print(\"Test score: \", grid_search.score(X_test_selected, y_test))\n",
    "        \n",
    "        print(\"Mean test score: \", grid_search.cv_results_['mean_test_score'])\n",
    "        print(\"Mean fit time: \", grid_search.cv_results_['mean_fit_time'])\n",
    "        print(\"Mean score time: \", grid_search.cv_results_['mean_score_time'])\n",
    "                \n",
    "\n",
    "MLP_2(test_df.copy(),ml_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "my_first_time_series = [1, 3, 4, 2]\n",
    "my_second_time_series = [1, 2, 4, 2]\n",
    "my_third_time_series = [1, 2, 4, 2, 2]\n",
    "X = to_time_series_dataset([my_first_time_series,\n",
    "                                my_second_time_series,\n",
    "                                my_third_time_series])\n",
    "y = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def performance(impute_dict):\n",
    "    imputation_df = pd.DataFrame.from_dict({(outerKey, innerKey): values for outerKey, innerDict in impute_dict.items() for innerKey, values in innerDict.items()}).T\n",
    "    r2_df = pd.DataFrame(imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).max(axis=0),columns = ['r2'])\n",
    "    r2_df['Method'] = imputation_df.drop(['mae','mse','rmse'], axis=0,level=1).idxmax(axis=0).values\n",
    "    mae_df = pd.DataFrame(imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).min(axis=0),columns = ['mae'])\n",
    "    mae_df['Method'] = imputation_df.drop(['r2','mse','rmse'], axis=0,level=1).idxmin(axis=0).values\n",
    "    rmse_df = pd.DataFrame(imputation_df.drop(['r2','mse','mae'], axis=0,level=1).min(axis=0),columns = ['rmse'])\n",
    "    rmse_df['Method'] = imputation_df.drop(['r2','mse','mae'], axis=0,level=1).idxmin(axis=0).values\n",
    "    for row in r2_df.index:\n",
    "        print(row)\n",
    "        print(f\"Optimal Imputation Method for {row}: {r2_df.loc[row]['Method'][0]}, R2 score: {round(r2_df.loc[row]['r2'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {mae_df.loc[row]['Method'][0]}, MAE score: {round(mae_df.loc[row]['mae'],5)}\")\n",
    "        print(f\"Optimal Imputation Method for {row}: {rmse_df.loc[row]['Method'][0]}, RMSE score: {round(rmse_df.loc[row]['rmse'],5)}\\n\")\n",
    "\n",
    "    return imputation_df\n",
    "\n",
    "# print(f\"\\nImputation methods for {file} and error metrics in {year}, {month} modeled with NaN values in {year_2}, {month_2}.\\nIncluding gaps smaller than {nan_gap} seconds.\\n\")\n",
    "performance(imputation_dict.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
